{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load(r'C:\\Users\\c\\Desktop\\KAIST 기초기계학습\\all_reg_arr.npz',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array1 = data['x_arr']\n",
    "array2 = data['y_arr']\n",
    "array3 = data['c_arr']\n",
    "array4 = data['a_arr']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "array1.index -> 0:ECG, 1:PPG , 2:CO2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = array1\n",
    "y = array2\n",
    "c = array3\n",
    "a = array4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[~np.isnan(array2)]\n",
    "y = y[~np.isnan(array2)]\n",
    "c = c[~np.isnan(array2)]\n",
    "a = a[~np.isnan(array2)]\n",
    "\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(c.shape)\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[:,:,1].reshape(-1,1,3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x에서 nonnan,0이상으로만 구성된 데이터 idx추출\n",
    "idx = []\n",
    "for i in range(len(x)):\n",
    "    if (np.isnan(x[i]).sum() == 0)&(np.min(x[i]) > 0):\n",
    "        idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 필터 및 전처리\n",
    "from pyvital import arr\n",
    "\n",
    "def process_beat(seg):\n",
    "    data = arr.interp_undefined(seg.flatten())\n",
    "    # beat detection\n",
    "    minlist, maxlist = arr.detect_peaks(data, 100)\n",
    "    maxlist = maxlist[1:]\n",
    "    # caseid = caseid.replace(‘.vital’, ‘’)\n",
    "    # beat lengths\n",
    "    beatlens = []\n",
    "    beats = []\n",
    "    beats_128 = []\n",
    "    beats_128_valid = []\n",
    "    for j in range(0, len(minlist)-1):\n",
    "        beatlen = minlist[j+1] - minlist[j]  # in samps\n",
    "        beatlens.append(beatlen)\n",
    "        beat = data[minlist[j]:minlist[j+1]]\n",
    "        beats.append(beat)\n",
    "        resampled = arr.resample(beat, 100)  # 임시로 100\n",
    "        beats_128.append(resampled)\n",
    "        beats_128_valid.append(resampled)\n",
    "    avgbeat = np.array(beats_128_valid).mean(axis=0)\n",
    "    nucase_mbeats = len(maxlist)\n",
    "    if nucase_mbeats < 10:\n",
    "        # print(‘nucase_mbeats < 4’)\n",
    "        return 0, []\n",
    "    meanlen = np.mean(beatlens)\n",
    "    stdlen = np.std(beatlens)\n",
    "    if stdlen > meanlen * 0.2: # irregular rhythm\n",
    "        # print(‘irregular rhythm’)\n",
    "        return 0, []\n",
    "    beatstds = []\n",
    "    for i in range(len(beats_128)):\n",
    "        if np.corrcoef(avgbeat, beats_128[i])[0, 1] < 0.9:\n",
    "            # print(‘corrcoef’)\n",
    "            return 0, []\n",
    "        else:\n",
    "            beatstds.append(np.std(beats[i]))\n",
    "        return np.mean(beatstds), []\n",
    "\n",
    "def scale_data(data): # ppg scale\n",
    "    rng = np.nanmax(data) - np.nanmin(data)\n",
    "    minimum = np.nanmin(data)\n",
    "    data = (data - minimum) / rng\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x[idx]\n",
    "y = y[idx]\n",
    "c = c[idx]\n",
    "a = a[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = x.astype(float)\n",
    "y = y.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_mstds_list = []\n",
    "for i in range(len(x)):\n",
    "    mstd_seg, _ = process_beat(x[i].reshape(-1, 1))\n",
    "    case_mstds_list.append(mstd_seg)\n",
    "case_mask = list(np.where(np.array(np.squeeze(case_mstds_list)) > 0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_arr = x[case_mask]\n",
    "y_arr = y[case_mask]\n",
    "c_arr = c[case_mask]\n",
    "import pickle\n",
    "pickle.dump((x_arr, y_arr, c_arr), open('processed_data.pkl', 'wb'))\n",
    "#x_arr, y_arr, c_arr  = pickle.load(open('processed_data.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "x_arr, y_arr, c_arr  = pickle.load(open('reg_processed_data.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "\n",
    "x_scaled = np.empty_like(x_arr)\n",
    "\n",
    "Scaler = MinMaxScaler()\n",
    "for i in range(x_arr.shape[0]):\n",
    "    x_scaled[i,:] = Scaler.fit_transform(x_arr[i,:].reshape(-1,1)).flatten()'''\n",
    "\n",
    "def scale_data(data): # ppg scale\n",
    "    rng = np.nanmax(data) - np.nanmin(data)\n",
    "    minimum = np.nanmin(data)\n",
    "    data = (data - minimum) / rng\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('reg_train_valid_test_caseids.pkl','rb') as file:\n",
    "    caseids = pickle.load(file)\n",
    "\n",
    "print(caseids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.isin(c_arr,caseids[0])\n",
    "valid_mask = np.isin(c_arr,caseids[1])\n",
    "test_mask = np.isin(c_arr,caseids[2])\n",
    "\n",
    "print(train_mask.sum())\n",
    "print(valid_mask.sum())\n",
    "print(test_mask.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_arr[train_mask]\n",
    "y_train = y_arr[train_mask]\n",
    "c_train = c_arr[train_mask]\n",
    "\n",
    "x_valid = x_arr[valid_mask]\n",
    "y_valid = y_arr[valid_mask]\n",
    "c_valid = c_arr[valid_mask]\n",
    "\n",
    "x_test = x_arr[test_mask]\n",
    "y_test = y_arr[test_mask]\n",
    "c_test = c_arr[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    x_train[i,0,:] = scale_data(x_train[i,0,:]) \n",
    "\n",
    "for i in range(len(x_valid)):\n",
    "    x_valid[i,0,:] = scale_data(x_valid[i,0,:]) \n",
    "\n",
    "for i in range(len(x_test)):\n",
    "    x_test[i,0,:] = scale_data(x_test[i,0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"caseids train cnt: {}, caseids val cnt {}, caseids test cnt: {}\".format(len(caseids[0]), len(caseids[1]), len(caseids[2])))\n",
    "print(\"samples train cnt: {}, samples val cnt: {}, samples test cnt: {}\".format(len(x_train), len(x_valid), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x:',x_train.shape,'y:',y_train.shape)\n",
    "print('x:',x_valid.shape,'y:',y_valid.shape)\n",
    "print('x:',x_test.shape,'y:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_train = np.expand_dims(y_train,axis=-1)\n",
    "y_valid = np.expand_dims(y_valid,axis=-1)\n",
    "y_test = np.expand_dims(y_test,axis=-1)\n",
    "\n",
    "print('x:',x_train.shape,'y:',y_train.shape)\n",
    "print('x:',x_valid.shape,'y:',y_valid.shape)\n",
    "print('x:',x_test.shape,'y:',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensors = torch.tensor(x_train,dtype=torch.float32)\n",
    "x_valid_tensors = torch.tensor(x_valid,dtype=torch.float32)\n",
    "x_test_tensors = torch.tensor(x_test,dtype=torch.float32)\n",
    "\n",
    "x_train_tensors = x_train_tensors.permute(0,2,1)\n",
    "x_valid_tensors = x_valid_tensors.permute(0,2,1)\n",
    "x_test_tensors = x_test_tensors.permute(0,2,1)\n",
    "\n",
    "y_train_tensors = torch.tensor(y_train,dtype=torch.float32)\n",
    "y_valid_tensors = torch.tensor(y_valid,dtype=torch.float32)\n",
    "y_test_tensors = torch.tensor(y_test,dtype=torch.float32)\n",
    "\n",
    "#single-channel\n",
    "#X_train_tensors_f = torch.reshape(X_train_tensors,())\n",
    "\n",
    "print(\"Training Shape\", x_train_tensors.shape, y_train_tensors.shape)\n",
    "print(\"Testing Shape\", x_test_tensors.shape, y_test_tensors.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "        return sample, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(x_train_tensors,y_train_tensors)\n",
    "valid_dataset = CustomDataset(x_valid_tensors,y_valid_tensors)\n",
    "test_dataset = CustomDataset(x_test_tensors,y_test_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(dataset=train_dataset,batch_size = 128, shuffle=True)\n",
    "valid_loader = DataLoader(dataset=valid_dataset,batch_size = 128, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size = 128, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.num_classes = num_classes \n",
    "        self.num_layers = num_layers \n",
    "        self.input_size = input_size \n",
    "        self.hidden_size = hidden_size \n",
    "        self.seq_length = seq_length \n",
    "\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) \n",
    "        self.fc_1 =  nn.Linear(hidden_size, 128) \n",
    "        self.fc = nn.Linear(64, num_classes)\n",
    "        self.fc_2 = nn.Linear(128,64) \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(x.device)\n",
    "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)).to(x.device)\n",
    "        \n",
    "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) \n",
    "        #rint(hn[-1].shape)\n",
    "        hn = hn[-1].view(-1 , self.hidden_size)\n",
    "        #print(hn.shape)\n",
    "\n",
    "        out = self.relu(hn)\n",
    "        out = self.fc_1(out) \n",
    "        out = self.relu(out)\n",
    "        out = self.fc_2(out)\n",
    "        out = self.relu(out) \n",
    "        out = self.fc(out)\n",
    "        \n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100 \n",
    "learning_rate = 1e-3 \n",
    "\n",
    "input_size = 1 \n",
    "hidden_size = 256 \n",
    "num_layers = 3 \n",
    "\n",
    "num_classes = 1 \n",
    "model = LSTM(num_classes, input_size, hidden_size, num_layers, x_train_tensors.shape[1]) \n",
    "\n",
    "criterion = torch.nn.MSELoss() #or L1.loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(torch.cuda.is_available())\n",
    "model = model.to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytorch Early-stopping\n",
    "import torch\n",
    "import numpy as np \n",
    " \n",
    " \n",
    "class EarlyStopping(object):\n",
    "    def __init__(self, patience=5, save_path=\"model.pth\"):\n",
    "        self._min_loss = np.inf\n",
    "        self._patience = patience\n",
    "        self._path = save_path\n",
    "        self.__counter = 0\n",
    " \n",
    "    def should_stop(self, model, loss):\n",
    "        if loss < self._min_loss:\n",
    "            self._min_loss = loss\n",
    "            self.__counter = 0\n",
    "            torch.save(model.state_dict(), self._path)\n",
    "        elif loss > self._min_loss:\n",
    "            self.__counter += 1\n",
    "            if self.__counter >= self._patience:\n",
    "                return True\n",
    "        return False\n",
    "   \n",
    "    def load(self, model):\n",
    "        model.load_state_dict(torch.load(self._path))\n",
    "        return model\n",
    "    \n",
    "    @property\n",
    "    def counter(self):\n",
    "        return self.__counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping_score(object):\n",
    "    \"\"\"score로 stopping하기\"\"\"\n",
    "    def __init__(self, patience, save_path, eps):\n",
    "        self._max_score = -1\n",
    "        self._patience = patience\n",
    "        self._path = save_path\n",
    "        self._eps = eps\n",
    "        self.__counter = 0\n",
    " \n",
    "    def should_stop(self, model, score):\n",
    "        if score > self._max_score:\n",
    "            self._max_score = score\n",
    "            self.__counter = 0\n",
    "            torch.save(model.state_dict(), self._path)\n",
    "        elif score < self._max_score + self._eps:\n",
    "            self.__counter += 1\n",
    "            if self.__counter >= self._patience:\n",
    "                return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "early_stopper = EarlyStopping(patience=4)\n",
    "\n",
    "\n",
    "train_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
    "        for x_train, y_train in tepoch:\n",
    "            tepoch.set_description(f\"Epoch {epoch+1}\")\n",
    "            x_train, y_train = x_train.to(device), y_train.reshape(-1,1).to(device) \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_train)\n",
    "                    \n",
    "            loss = criterion(y_train,outputs)\n",
    "            train_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "\n",
    "            tepoch.set_postfix(loss=loss.item())\n",
    "        \n",
    "        train_loss = train_loss / len(train_loader)\n",
    "    \n",
    "    \n",
    "    all_probs = []\n",
    "    all_labels = []\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for i, batch in enumerate(tqdm(valid_loader)):\n",
    "            x_valid, y_valid = batch\n",
    "            x_valid, y_valid = x_valid.to(device), y_valid.to(device) \n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x_valid)\n",
    "                    \n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "            all_labels.extend(y_valid.cpu().numpy())\n",
    "            \n",
    "    diff_y = [i-j for i, j in zip(all_labels, all_probs)]\n",
    "    mae = np.mean(np.abs(diff_y))\n",
    "    r2 = r2_score(all_labels, all_probs)\n",
    "\n",
    "    if early_stopper.should_stop(model,train_loss):\n",
    "        print(f'Early Stopping')\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}],Train loss:{train_loss:.3f}, Validation MAE: {mae:.3f}%, R2_score:{r2:.3f}')\n",
    "        torch.save(model.state_dict(), './best_lstm_reg.pt')\n",
    "\n",
    "\n",
    "        break\n",
    "\n",
    "    print(\"MAE Score:\", mae)\n",
    "\n",
    "    # auc_score = roc_auc_score(all_labels, all_probs)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Train loss:{train_loss:.3f},Validation MAE: {mae:.3f}%, R2_score:{r2:.3f}')\n",
    "    \n",
    "    \n",
    "model.eval()\n",
    "\n",
    "# 예측 확률을 저장할 리스트\n",
    "test_probs = []\n",
    "test_labels = []\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(tqdm(test_loader)):\n",
    "        x_test, y_test = batch\n",
    "        x_test, y_test = x_test.to(device), y_test.to(device) \n",
    "        \n",
    "        outputs = model(x_test)\n",
    "        test_probs.extend(outputs.cpu().numpy())\n",
    "        test_labels.extend(y_test.cpu().numpy())\n",
    "\n",
    "diff_y = [i-j for i, j in zip(y_test.cpu().numpy(), test_probs)]\n",
    "r2 = r2_score(test_labels,test_probs)\n",
    "\n",
    "mae = np.mean(np.abs(diff_y))\n",
    "print(f'Test MAE: {mae:.3f} Test R2:{r2:.3f}')\n",
    "pickle.dump((all_probs,all_labels), open('./result/reg/lstm_reg_valid_pred_true.pkl', 'wb'))\n",
    "pickle.dump((test_probs,test_labels), open('./result/reg/lstm_reg_test_pred_true.pkl', 'wb'))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
