{"cells":[{"cell_type":"markdown","metadata":{"id":"gu1mnQlMjdGU"},"source":["## Train"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23245,"status":"ok","timestamp":1702895281163,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"JchSeTWxjhDp","outputId":"ade377cf-4f87-4251-bbe4-b1718428c1d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["# from google.colab import drive\n","# drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":415,"status":"ok","timestamp":1702895703880,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"TASed7rMOT9g","outputId":"ba5be2db-c1da-418e-cf1d-7b8251a4a878"},"outputs":[],"source":["!pwd"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1702895704734,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"MxNQL_h9OY9j","outputId":"86bc14c2-e080-40d6-e6f4-26c32382a03e"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Colab Notebooks/[Microdegree] Hypotension/code\n","/content/drive/MyDrive/Colab Notebooks/[Microdegree] Hypotension/code\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/[Microdegree] Hypotension/code # change directory\n","!pwd"]},{"cell_type":"markdown","metadata":{"id":"-MliqGs3jdGY"},"source":["### Train"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1702895329262,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"OdYaDoKiNiA4"},"outputs":[],"source":["# !pip install torchsummary\n","# !pip install tensorboard\n","# !pip install wandb"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":17679,"status":"ok","timestamp":1702895724706,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"C1qwNaX3NiA6"},"outputs":[],"source":["import numpy as np\n","from collections import Counter\n","from tqdm import tqdm\n","from matplotlib import pyplot as plt\n","\n","import os\n","import glob\n","import re\n","import pickle\n","import multiprocessing\n","import wandb\n","import argparse\n","from datetime import datetime\n","from pathlib import Path\n","import random\n","import json\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchsummary import summary\n","\n","from model_resnet1d import ResNet1D\n","from pytorchtools import EarlyStopping\n","\n","# from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, accuracy_score, precision_score, recall_score, f1_score # classification\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # regression"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":495,"status":"ok","timestamp":1702895736293,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"tw4DIOzDNiA7","outputId":"8f794a08-86f0-4fd9-cd35-586f18e0fbf3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Namespace(seed=42, epochs=100, batch_size=128, lr=0.001, criterion='mean_squared', log_interval=200, model='resnet1d', name='exp', model_dir='./model_reg')\n"]}],"source":["def seed_everything(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","def increment_path(path, exist_ok=False):\n","    \"\"\" Automatically increment path, i.e. runs/exp --> runs/exp0, runs/exp1 etc.\n","\n","    Args:\n","        path (str or pathlib.Path): f\"{model_dir}/{args.name}\".\n","        exist_ok (bool): whether increment path (increment if False).\n","    \"\"\"\n","    path = Path(path)\n","    if (path.exists() and exist_ok) or (not path.exists()):\n","        return str(path)\n","    else:\n","        dirs = glob.glob(f\"{path}*\")\n","        matches = [re.search(rf\"%s(\\d+)\" % path.stem, d) for d in dirs]\n","        i = [int(m.groups()[0]) for m in matches if m]\n","        n = max(i) + 1 if i else 2\n","        return f\"{path}{n}\"\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","\n","_criterion_entrypoints = {\n","    'mean_squared': nn.MSELoss,\n","    'l1' : nn.L1Loss,\n","}\n","\n","def criterion_entrypoint(criterion_name):\n","    return _criterion_entrypoints[criterion_name]\n","\n","def is_criterion(criterion_name):\n","    return criterion_name in _criterion_entrypoints\n","\n","def create_criterion(criterion_name, **kwargs):\n","    if is_criterion(criterion_name):\n","        create_fn = criterion_entrypoint(criterion_name)\n","        criterion = create_fn(**kwargs)\n","    else:\n","        raise RuntimeError('Unknown loss (%s)' % criterion_name)\n","    return criterion\n","\n","now = datetime.now()\n","folder_name = now.strftime('%Y-%m-%d-%H:%M:%S')\n","parser = argparse.ArgumentParser()\n","\n","parser.add_argument('--seed', type=int, default=42, help='random seed (default: 42)')\n","parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train (default: 1)')\n","parser.add_argument('--batch_size', type=int, default=128, help='input batch size for training (default: 64)')\n","parser.add_argument('--lr', type=float, default=1e-3, help='learning rate (default: 1e-3)')\n","# parser.add_argument('--lr_decay_step', type=int, default=20, help='learning rate scheduler deacy step (default: 20)')\n","parser.add_argument('--criterion', type=str, default='mean_squared', help='criterion type (default: cross_entropy)')\n","parser.add_argument('--log_interval', type=int, default=200, help='how many batches to wait before logging training status')\n","parser.add_argument('--model', type=str, default='resnet1d', help='model type (default: BaseModel)')\n","parser.add_argument('--name', default='exp', help='model save at {SM_MODEL_DIR}/{name}')\n","# parser.add_argument('--name', default='exp_'+folder_name, help='model save at {SM_MODEL_DIR}/{name}')\n","parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', './model_reg'))\n","\n","# args = parser.parse_args()\n","args, _ = parser.parse_known_args()\n","print(args)"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":286,"status":"ok","timestamp":1702895738642,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"oudZ8_lONiA9","outputId":"9ed4ff04-ebec-4dcd-c029-cfa8b18240ff"},"outputs":[{"name":"stdout","output_type":"stream","text":["./model_reg\n","model_reg\\mean_squared_epoch100_batch128_exp\n"]}],"source":["model_dir = args.model_dir\n","# save_dir = increment_path(os.path.join(model_dir, args.name))\n","# save_dir = increment_path(os.path.join(model_dir, args.name+\"_\"+args.criterion+\"_\"+str(args.epochs)))\n","save_dir = increment_path(os.path.join(model_dir, args.criterion+\"_epoch\"+str(args.epochs)+\"_batch\"+str(args.batch_size)+\"_\"+args.name))\n","\n","print(model_dir)\n","print(save_dir)"]},{"cell_type":"markdown","metadata":{"id":"Hlf3TO59jdGb"},"source":["### # DataLoader"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":343,"status":"ok","timestamp":1702895740878,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"Kjo2ROfwNiA9"},"outputs":[],"source":["class PPGDataset(Dataset):\n","    def __init__(self, data, label):\n","        self.data = data\n","        self.label = label\n","\n","    def __getitem__(self, index):\n","        # return (torch.tensor(self.data[index], dtype=torch.float), torch.tensor(self.label[index], dtype=torch.long)) # torch.long\n","        return (torch.as_tensor(self.data[index], dtype=torch.float), torch.as_tensor(self.label[index], dtype=torch.long)) # torch.long\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"uSKfSJChNiA_"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'torch.Tensor'> <class 'torch.Tensor'>\n","torch.Size([292544, 1, 3000]) torch.Size([292544, 1])\n","1 tensor([[0.0982, 0.1071, 0.0982,  ..., 0.1161, 0.1071, 0.1161]])\n","\n","<class 'torch.Tensor'> <class 'torch.Tensor'>\n","torch.Size([95515, 1, 3000]) torch.Size([95515, 1])\n","1 tensor([[0.0541, 0.0676, 0.0541,  ..., 0.5135, 0.4730, 0.4730]])\n","\n","<class 'torch.Tensor'> <class 'torch.Tensor'>\n","torch.Size([95880, 1, 3000]) torch.Size([95880, 1])\n","1 tensor([[0.0849, 0.0755, 0.0755,  ..., 0.0849, 0.0849, 0.0849]])\n","<__main__.PPGDataset object at 0x000002495C7C2AD0>\n","<__main__.PPGDataset object at 0x000002495C7D5890>\n","<__main__.PPGDataset object at 0x000002495C78DD50>\n"]}],"source":["# get dataset\n","with open('../data_reg/train_reg_scaled_x.pkl', 'rb') as f:\n","    # train_X = pickle.load(f)\n","    X = pickle.load(f)\n","    train_X = np.expand_dims(X, 1)\n","    train_X = torch.tensor(train_X, dtype = torch.float32)\n","with open('../data_reg/train_reg_y.pkl', 'rb') as f:\n","    # train_Y = pickle.load(f)\n","    Y = pickle.load(f)\n","    train_Y = np.expand_dims(Y, 1)\n","    train_Y = torch.tensor(train_Y, dtype = torch.float32)\n","\n","with open('../data_reg/valid_reg_scaled_x.pkl', 'rb') as f:\n","    # valid_X = pickle.load(f)\n","    X = pickle.load(f)\n","    valid_X = np.expand_dims(X, 1)\n","    valid_X = torch.tensor(valid_X, dtype = torch.float32)\n","with open('../data_reg/valid_reg_y.pkl', 'rb') as f:\n","    # valid_Y = pickle.load(f)\n","    Y = pickle.load(f)\n","    valid_Y = np.expand_dims(Y, 1)\n","    valid_Y = torch.tensor(valid_Y, dtype = torch.float32)\n","\n","with open('../data_reg/test_reg_scaled_x.pkl', 'rb') as f:\n","    # test_X = pickle.load(f)\n","    X = pickle.load(f)\n","    test_X = np.expand_dims(X, 1)\n","    test_X = torch.tensor(test_X, dtype = torch.float32)\n","with open('../data_reg/test_reg_y.pkl', 'rb') as f:\n","    # test_Y = pickle.load(f)\n","    Y = pickle.load(f)\n","    test_Y = np.expand_dims(Y, 1)\n","    test_Y = torch.tensor(test_Y, dtype = torch.float32)\n","\n","train_dataset = PPGDataset(train_X, train_Y)\n","val_dataset = PPGDataset(valid_X, valid_Y)\n","test_dataset = PPGDataset(test_X, test_Y)\n","\n","print(type(train_X), type(train_Y))\n","print(train_X.shape, train_Y.shape)\n","# print(len(train_X), len(train_Y))\n","print(len(train_X[0]), train_X[0])\n","# print(train_Y[:20])\n","print()\n","print(type(valid_X), type(valid_Y))\n","print(valid_X.shape, valid_Y.shape)\n","# print(len(valid_X), len(valid_Y))\n","print(len(valid_X[0]), valid_X[0])\n","# print(valid_Y[:20])\n","print()\n","print(type(test_X), type(test_Y))\n","print(test_X.shape, test_Y.shape)\n","# # print(len(test_X), len(test_Y))\n","print(len(test_X[0]), test_X[0])\n","# # print(test_Y[:20])\n","print()\n","\n","print(train_dataset)\n","print(val_dataset)\n","print(test_dataset)\n","\n","# <class 'torch.Tensor'> <class 'torch.Tensor'>\n","# torch.Size([292544, 1, 3000]) torch.Size([292544, 1])\n","\n","# <class 'torch.Tensor'> <class 'torch.Tensor'>\n","# torch.Size([95515, 1, 3000]) torch.Size([95515, 1])\n","\n","# <class 'torch.Tensor'> <class 'torch.Tensor'>\n","# torch.Size([95880, 1, 3000]) torch.Size([95880, 1])"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1702060788528,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"W8_nv7PBNiBA","outputId":"554e7229-0674-46fe-bcef-862b2f39127d"},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n","<torch.utils.data.dataloader.DataLoader object at 0x0000024942B02D50>\n","<torch.utils.data.dataloader.DataLoader object at 0x0000024954608210>\n","<torch.utils.data.dataloader.DataLoader object at 0x000002494165F390>\n"]}],"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=args.batch_size,\n","    num_workers=multiprocessing.cpu_count() // 2,\n","    shuffle=True,\n","    pin_memory=use_cuda,\n","    drop_last=True,\n","    )\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=args.batch_size,\n","    num_workers=multiprocessing.cpu_count() // 2,\n","    shuffle=False,\n","    pin_memory=use_cuda,\n","    drop_last=True,\n","    )\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    shuffle=False\n",")\n","\n","print(device)\n","print(train_dataloader)\n","print(val_dataloader)\n","print(test_dataloader)"]},{"cell_type":"markdown","metadata":{"id":"RQuaTyv5jdGc"},"source":["### # wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1702060792808,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"rGfN2C0NNiBB","outputId":"f99c2a8b-1081-48a2-8ded-8b9e7f3b1e22"},"outputs":[{"data":{"text/plain":["True"]},"execution_count":22,"metadata":{},"output_type":"execute_result"}],"source":["import os\n","os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"notebook name here\"\n","\n","# !pip install wandb -qqq\n","import wandb\n","wandb.login()\n","# !wandb login --relogin"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"elapsed":2056,"status":"ok","timestamp":1702060796226,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"1mdVZjZ-NiBB","outputId":"fc9b7e5b-739b-43d8-8809-26d6bcbbf200"},"outputs":[{"data":{"text/html":["Tracking run with wandb version 0.16.1"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/[Microdegree] Hypotension/code/wandb/run-20231208_183953-gq3p7aj9</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9' target=\"_blank\">exp_2023-12-08-18:39:34resnet1d100</a></strong> to <a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View project at <a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension' target=\"_blank\">https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":[" View run at <a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9' target=\"_blank\">https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9</a>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7e37a22411b0>"]},"execution_count":23,"metadata":{},"output_type":"execute_result"}],"source":["# -- wandb initialize with configuration\n","config={\n","    \"epochs\": args.epochs,\n","    \"batch_size\": args.batch_size,\n","    \"learning_rate\" : args.lr,\n","    \"architecture\" : args.model,\n","    \"loss\" : args.criterion\n","}\n","wandb.init(project=\"KAIST GSDS Microdegree - Hypotension\", name = str(save_dir.split('/')[-1])+str(args.model)+str(args.epochs), config=config)"]},{"cell_type":"markdown","metadata":{"id":"K6DBeMa8jdGd"},"source":["### # Model"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":847,"status":"ok","timestamp":1702060804118,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"MD3AjihTNiBC","outputId":"50fe7c7f-13e8-4394-f097-6b0eebdf1e0a"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([292544, 1, 3000]) torch.Size([292544, 1])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv1d-1            [-1, 128, 3000]           2,176\n","   MyConv1dPadSame-2            [-1, 128, 3000]               0\n","       BatchNorm1d-3            [-1, 128, 3000]             256\n","              ReLU-4            [-1, 128, 3000]               0\n","            Conv1d-5            [-1, 128, 3000]           8,320\n","   MyConv1dPadSame-6            [-1, 128, 3000]               0\n","       BatchNorm1d-7            [-1, 128, 3000]             256\n","              ReLU-8            [-1, 128, 3000]               0\n","           Dropout-9            [-1, 128, 3000]               0\n","           Conv1d-10            [-1, 128, 3000]           8,320\n","  MyConv1dPadSame-11            [-1, 128, 3000]               0\n","       BasicBlock-12            [-1, 128, 3000]               0\n","      BatchNorm1d-13            [-1, 128, 3000]             256\n","             ReLU-14            [-1, 128, 3000]               0\n","          Dropout-15            [-1, 128, 3000]               0\n","           Conv1d-16            [-1, 128, 1500]           8,320\n","  MyConv1dPadSame-17            [-1, 128, 1500]               0\n","      BatchNorm1d-18            [-1, 128, 1500]             256\n","             ReLU-19            [-1, 128, 1500]               0\n","          Dropout-20            [-1, 128, 1500]               0\n","           Conv1d-21            [-1, 128, 1500]           8,320\n","  MyConv1dPadSame-22            [-1, 128, 1500]               0\n","        MaxPool1d-23            [-1, 128, 1500]               0\n","MyMaxPool1dPadSame-24            [-1, 128, 1500]               0\n","       BasicBlock-25            [-1, 128, 1500]               0\n","      BatchNorm1d-26            [-1, 128, 1500]             256\n","             ReLU-27            [-1, 128, 1500]               0\n","          Dropout-28            [-1, 128, 1500]               0\n","           Conv1d-29            [-1, 128, 1500]           8,320\n","  MyConv1dPadSame-30            [-1, 128, 1500]               0\n","      BatchNorm1d-31            [-1, 128, 1500]             256\n","             ReLU-32            [-1, 128, 1500]               0\n","          Dropout-33            [-1, 128, 1500]               0\n","           Conv1d-34            [-1, 128, 1500]           8,320\n","  MyConv1dPadSame-35            [-1, 128, 1500]               0\n","       BasicBlock-36            [-1, 128, 1500]               0\n","      BatchNorm1d-37            [-1, 128, 1500]             256\n","             ReLU-38            [-1, 128, 1500]               0\n","          Dropout-39            [-1, 128, 1500]               0\n","           Conv1d-40             [-1, 128, 750]           8,320\n","  MyConv1dPadSame-41             [-1, 128, 750]               0\n","      BatchNorm1d-42             [-1, 128, 750]             256\n","             ReLU-43             [-1, 128, 750]               0\n","          Dropout-44             [-1, 128, 750]               0\n","           Conv1d-45             [-1, 128, 750]           8,320\n","  MyConv1dPadSame-46             [-1, 128, 750]               0\n","        MaxPool1d-47             [-1, 128, 750]               0\n","MyMaxPool1dPadSame-48             [-1, 128, 750]               0\n","       BasicBlock-49             [-1, 128, 750]               0\n","      BatchNorm1d-50             [-1, 128, 750]             256\n","             ReLU-51             [-1, 128, 750]               0\n","          Dropout-52             [-1, 128, 750]               0\n","           Conv1d-53             [-1, 256, 750]          16,640\n","  MyConv1dPadSame-54             [-1, 256, 750]               0\n","      BatchNorm1d-55             [-1, 256, 750]             512\n","             ReLU-56             [-1, 256, 750]               0\n","          Dropout-57             [-1, 256, 750]               0\n","           Conv1d-58             [-1, 256, 750]          33,024\n","  MyConv1dPadSame-59             [-1, 256, 750]               0\n","       BasicBlock-60             [-1, 256, 750]               0\n","      BatchNorm1d-61             [-1, 256, 750]             512\n","             ReLU-62             [-1, 256, 750]               0\n","          Dropout-63             [-1, 256, 750]               0\n","           Conv1d-64             [-1, 256, 375]          33,024\n","  MyConv1dPadSame-65             [-1, 256, 375]               0\n","      BatchNorm1d-66             [-1, 256, 375]             512\n","             ReLU-67             [-1, 256, 375]               0\n","          Dropout-68             [-1, 256, 375]               0\n","           Conv1d-69             [-1, 256, 375]          33,024\n","  MyConv1dPadSame-70             [-1, 256, 375]               0\n","        MaxPool1d-71             [-1, 256, 375]               0\n","MyMaxPool1dPadSame-72             [-1, 256, 375]               0\n","       BasicBlock-73             [-1, 256, 375]               0\n","      BatchNorm1d-74             [-1, 256, 375]             512\n","             ReLU-75             [-1, 256, 375]               0\n","          Dropout-76             [-1, 256, 375]               0\n","           Conv1d-77             [-1, 256, 375]          33,024\n","  MyConv1dPadSame-78             [-1, 256, 375]               0\n","      BatchNorm1d-79             [-1, 256, 375]             512\n","             ReLU-80             [-1, 256, 375]               0\n","          Dropout-81             [-1, 256, 375]               0\n","           Conv1d-82             [-1, 256, 375]          33,024\n","  MyConv1dPadSame-83             [-1, 256, 375]               0\n","       BasicBlock-84             [-1, 256, 375]               0\n","      BatchNorm1d-85             [-1, 256, 375]             512\n","             ReLU-86             [-1, 256, 375]               0\n","          Dropout-87             [-1, 256, 375]               0\n","           Conv1d-88             [-1, 256, 188]          33,024\n","  MyConv1dPadSame-89             [-1, 256, 188]               0\n","      BatchNorm1d-90             [-1, 256, 188]             512\n","             ReLU-91             [-1, 256, 188]               0\n","          Dropout-92             [-1, 256, 188]               0\n","           Conv1d-93             [-1, 256, 188]          33,024\n","  MyConv1dPadSame-94             [-1, 256, 188]               0\n","        MaxPool1d-95             [-1, 256, 188]               0\n","MyMaxPool1dPadSame-96             [-1, 256, 188]               0\n","       BasicBlock-97             [-1, 256, 188]               0\n","      BatchNorm1d-98             [-1, 256, 188]             512\n","             ReLU-99             [-1, 256, 188]               0\n","         Dropout-100             [-1, 256, 188]               0\n","          Conv1d-101             [-1, 512, 188]          66,048\n"," MyConv1dPadSame-102             [-1, 512, 188]               0\n","     BatchNorm1d-103             [-1, 512, 188]           1,024\n","            ReLU-104             [-1, 512, 188]               0\n","         Dropout-105             [-1, 512, 188]               0\n","          Conv1d-106             [-1, 512, 188]         131,584\n"," MyConv1dPadSame-107             [-1, 512, 188]               0\n","      BasicBlock-108             [-1, 512, 188]               0\n","     BatchNorm1d-109             [-1, 512, 188]           1,024\n","            ReLU-110             [-1, 512, 188]               0\n","         Dropout-111             [-1, 512, 188]               0\n","          Conv1d-112              [-1, 512, 94]         131,584\n"," MyConv1dPadSame-113              [-1, 512, 94]               0\n","     BatchNorm1d-114              [-1, 512, 94]           1,024\n","            ReLU-115              [-1, 512, 94]               0\n","         Dropout-116              [-1, 512, 94]               0\n","          Conv1d-117              [-1, 512, 94]         131,584\n"," MyConv1dPadSame-118              [-1, 512, 94]               0\n","       MaxPool1d-119              [-1, 512, 94]               0\n","MyMaxPool1dPadSame-120              [-1, 512, 94]               0\n","      BasicBlock-121              [-1, 512, 94]               0\n","     BatchNorm1d-122              [-1, 512, 94]           1,024\n","            ReLU-123              [-1, 512, 94]               0\n","         Dropout-124              [-1, 512, 94]               0\n","          Conv1d-125              [-1, 512, 94]         131,584\n"," MyConv1dPadSame-126              [-1, 512, 94]               0\n","     BatchNorm1d-127              [-1, 512, 94]           1,024\n","            ReLU-128              [-1, 512, 94]               0\n","         Dropout-129              [-1, 512, 94]               0\n","          Conv1d-130              [-1, 512, 94]         131,584\n"," MyConv1dPadSame-131              [-1, 512, 94]               0\n","      BasicBlock-132              [-1, 512, 94]               0\n","     BatchNorm1d-133              [-1, 512, 94]           1,024\n","            ReLU-134              [-1, 512, 94]               0\n","         Dropout-135              [-1, 512, 94]               0\n","          Conv1d-136              [-1, 512, 47]         131,584\n"," MyConv1dPadSame-137              [-1, 512, 47]               0\n","     BatchNorm1d-138              [-1, 512, 47]           1,024\n","            ReLU-139              [-1, 512, 47]               0\n","         Dropout-140              [-1, 512, 47]               0\n","          Conv1d-141              [-1, 512, 47]         131,584\n"," MyConv1dPadSame-142              [-1, 512, 47]               0\n","       MaxPool1d-143              [-1, 512, 47]               0\n","MyMaxPool1dPadSame-144              [-1, 512, 47]               0\n","      BasicBlock-145              [-1, 512, 47]               0\n","     BatchNorm1d-146              [-1, 512, 47]           1,024\n","            ReLU-147              [-1, 512, 47]               0\n","         Dropout-148              [-1, 512, 47]               0\n","          Conv1d-149             [-1, 1024, 47]         263,168\n"," MyConv1dPadSame-150             [-1, 1024, 47]               0\n","     BatchNorm1d-151             [-1, 1024, 47]           2,048\n","            ReLU-152             [-1, 1024, 47]               0\n","         Dropout-153             [-1, 1024, 47]               0\n","          Conv1d-154             [-1, 1024, 47]         525,312\n"," MyConv1dPadSame-155             [-1, 1024, 47]               0\n","      BasicBlock-156             [-1, 1024, 47]               0\n","     BatchNorm1d-157             [-1, 1024, 47]           2,048\n","            ReLU-158             [-1, 1024, 47]               0\n","         Dropout-159             [-1, 1024, 47]               0\n","          Conv1d-160             [-1, 1024, 24]         525,312\n"," MyConv1dPadSame-161             [-1, 1024, 24]               0\n","     BatchNorm1d-162             [-1, 1024, 24]           2,048\n","            ReLU-163             [-1, 1024, 24]               0\n","         Dropout-164             [-1, 1024, 24]               0\n","          Conv1d-165             [-1, 1024, 24]         525,312\n"," MyConv1dPadSame-166             [-1, 1024, 24]               0\n","       MaxPool1d-167             [-1, 1024, 24]               0\n","MyMaxPool1dPadSame-168             [-1, 1024, 24]               0\n","      BasicBlock-169             [-1, 1024, 24]               0\n","     BatchNorm1d-170             [-1, 1024, 24]           2,048\n","            ReLU-171             [-1, 1024, 24]               0\n","         Dropout-172             [-1, 1024, 24]               0\n","          Conv1d-173             [-1, 1024, 24]         525,312\n"," MyConv1dPadSame-174             [-1, 1024, 24]               0\n","     BatchNorm1d-175             [-1, 1024, 24]           2,048\n","            ReLU-176             [-1, 1024, 24]               0\n","         Dropout-177             [-1, 1024, 24]               0\n","          Conv1d-178             [-1, 1024, 24]         525,312\n"," MyConv1dPadSame-179             [-1, 1024, 24]               0\n","      BasicBlock-180             [-1, 1024, 24]               0\n","     BatchNorm1d-181             [-1, 1024, 24]           2,048\n","            ReLU-182             [-1, 1024, 24]               0\n","         Dropout-183             [-1, 1024, 24]               0\n","          Conv1d-184             [-1, 1024, 12]         525,312\n"," MyConv1dPadSame-185             [-1, 1024, 12]               0\n","     BatchNorm1d-186             [-1, 1024, 12]           2,048\n","            ReLU-187             [-1, 1024, 12]               0\n","         Dropout-188             [-1, 1024, 12]               0\n","          Conv1d-189             [-1, 1024, 12]         525,312\n"," MyConv1dPadSame-190             [-1, 1024, 12]               0\n","       MaxPool1d-191             [-1, 1024, 12]               0\n","MyMaxPool1dPadSame-192             [-1, 1024, 12]               0\n","      BasicBlock-193             [-1, 1024, 12]               0\n","     BatchNorm1d-194             [-1, 1024, 12]           2,048\n","            ReLU-195             [-1, 1024, 12]               0\n","          Linear-196                    [-1, 1]           1,025\n","================================================================\n","Total params: 5,276,033\n","Trainable params: 5,276,033\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 156.03\n","Params size (MB): 20.13\n","Estimated Total Size (MB): 176.17\n","----------------------------------------------------------------\n"]}],"source":["# make model\n","# device_str = \"cuda\"\n","# device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n","\n","## change the hyper-parameters for your own data\n","# (n_block, downsample_gap, increasefilter_gap) = (8, 1, 2)\n","# 34 layer (16*2+2): 16, 2, 4\n","# 98 layer (48*2+2): 48, 6, 12\n","\n","model = ResNet1D(\n","    in_channels=1, # 3000,\n","    base_filters=128, # 128, # 64 for ResNet1D, 352 for ResNeXt1D\n","    kernel_size= 16, # kernel_size,\n","    stride=2, # stride,\n","    groups=32,\n","    n_block=16, # 48, # n_block=48,\n","    n_classes=1, # 3, 4,\n","    downsample_gap=2, # 6, # downsample_gap,\n","    increasefilter_gap=4, # 12, # increasefilter_gap,\n","    use_do=True)\n","model.to(device)\n","print(train_X.shape, train_Y.shape)\n","summary(model, (train_X.shape[1], train_X.shape[2])) # device=device\n","# exit()\n","\n","model.verbose = False # True\n","optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.lr)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n","# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_decay_step, gamma=0.5)\n","# loss_func = torch.nn.CrossEntropyLoss()\n","criterion = create_criterion(args.criterion)  # default: cross_entropy\n","\n","logger = SummaryWriter(log_dir=save_dir)\n","with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n","    json.dump(vars(args), f, ensure_ascii=False, indent=4)\n"]},{"cell_type":"markdown","metadata":{"id":"F5aG7XXdjdGe"},"source":["### # Train"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3c4ddc2c4f1442009ade4bffe4204112","af2d459b88cc4a4b803be72c37647c9d","2c495edb24c94bd4bfbd580e415f7b79","919d878bd3444873b38bf986a8c62713","d148eb90e03d41e1a2d2e82feea94f46","1dc355d725bf447ab30d9e3a09ccb03c","3a1b8bbf44724051b66c67e05b43796d","3ec9b8836daf466e8b3fe6025a112e78"]},"executionInfo":{"elapsed":3493168,"status":"ok","timestamp":1702064304889,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"phNr5KmkChqI","outputId":"5503579d-08d5-487f-ac17-b5fa5143bea1"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore') # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n","\n","# early_stopping\n","early_stopping = EarlyStopping(patience = 7, verbose = True)\n","\n","# train\n","best_val_loss = np.inf\n","best_val_mae = 0\n","best_val_r2 = 0\n","\n","# for _ in tqdm(range(args.epochs), desc=\"epoch\", leave=False):\n","for epoch in range(args.epochs):\n","\n","    # train loop\n","    model.train()\n","\n","    loss_value = 0\n","    train_preds_by_batch = []\n","    train_labels_by_batch = []\n","\n","    # prog_iter = tqdm(train_dataloader, desc=\"Training\", leave=False)\n","    print(\"Training...\")\n","    # for batch_idx, train_batch in enumerate(prog_iter):\n","    for batch_idx, train_batch in enumerate(train_dataloader):\n","        input_x, input_y = tuple(t.to(device) for t in train_batch)\n","        # input_x, input_y = tuple(t for t in train_batch)\n","        # input_x = input_x.to(device).float()\n","        # input_y = input_y.to(device).long()\n","\n","        optimizer.zero_grad()\n","\n","        # preds = model(input_x)\n","        outs = model(input_x)\n","\n","        # print('outs : ', type(outs), outs.dtype, outs.shape, outs)\n","        # print('input_y : ', type(input_y), input_y.dtype, input_y.shape, input_y)\n","\n","        loss = criterion(outs.to(torch.float32), input_y.to(torch.float32)) # regression\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        loss_value += loss.item()\n","\n","        train_preds_by_batch.extend(outs.cpu().detach().numpy())\n","        train_labels_by_batch.extend(input_y.cpu().detach().numpy())\n","\n","        if (batch_idx + 1) % args.log_interval == 0:\n","            train_loss = loss_value / args.log_interval\n","\n","            train_mae = mean_absolute_error(train_labels_by_batch, train_preds_by_batch).item()\n","            train_mse = mean_squared_error(train_labels_by_batch, train_preds_by_batch).item()\n","            train_rmse = np.sqrt(train_mse)\n","            train_r2 = r2_score(train_labels_by_batch, train_preds_by_batch).item()\n","\n","            current_lr = get_lr(optimizer)\n","            print(\n","                f\"Epoch[{epoch + 1}/{args.epochs}]({batch_idx + 1}/{len(train_dataloader)}) || \"\n","                f\"training loss {train_loss:4.4} || training MAE {train_mae:4.4f} || training MSE {train_mse:4.4f} || training rMSE {train_rmse:4.4f} || training R2 {train_r2:4.4f} || lr {current_lr}\"\n","            )\n","            logger.add_scalar(\"Train/loss\", train_loss, epoch * len(train_dataloader) + batch_idx)\n","            logger.add_scalar(\"Train/mae\", train_mae, epoch * len(train_dataloader) + batch_idx)\n","            logger.add_scalar(\"Train/mse\", train_mse, epoch * len(train_dataloader) + batch_idx)\n","            logger.add_scalar(\"Train/rmse\", train_rmse, epoch * len(train_dataloader) + batch_idx)\n","            logger.add_scalar(\"Train/r2_score\", train_r2, epoch * len(train_dataloader) + batch_idx)\n","\n","            loss_value = 0\n","            train_preds_by_batch = []\n","            train_labels_by_batch = []\n","\n","\n","    # logging wandb train phase\n","    wandb.log({\n","        'Train loss': train_loss,\n","        'Train mae': train_mae,\n","        'Train mse': train_mse,\n","        'Train rmse': train_rmse,\n","        'Train r2': train_r2,\n","    })\n","    # scheduler.step(_)\n","    # scheduler.step(loss)\n","\n","    # val loop\n","    with torch.no_grad():\n","\n","        model.eval()\n","\n","        val_loss_items = []\n","        all_val_preds = []\n","        all_val_labels = []\n","\n","        # prog_iter_test = tqdm(val_dataloader, desc=\"Testing\", leave=False)\n","        print()\n","        print(\"Calculating validation results...\")\n","        # for batch_idx, val_batch in enumerate(prog_iter_test):\n","        for val_batch in val_dataloader:\n","            input_x, input_y = tuple(t.to(device) for t in val_batch)\n","            # input_x, input_y = tuple(t for t in val_batch)\n","            # input_x = input_x.to(device).float()\n","            # input_y = input_y.to(device).long()\n","\n","            outs = model(input_x)\n","\n","            loss_item = criterion(outs.to(torch.float32), input_y.to(torch.float32)).item() # regression\n","            \n","            val_loss_items.append(loss_item)\n","\n","            all_val_preds.extend(outs.cpu().detach().numpy())\n","            all_val_labels.extend(input_y.cpu().detach().numpy())\n","\n","    val_loss = np.sum(val_loss_items) / len(val_dataloader)\n","    best_val_loss = min(best_val_loss, val_loss)\n","\n","    val_mae = mean_absolute_error(all_val_labels, all_val_preds)\n","    val_mse = mean_squared_error(all_val_labels, all_val_preds)\n","    val_rmse = np.sqrt(val_mse)\n","    val_r2 = r2_score(all_val_labels, all_val_preds)\n","    best_val_mae = max(best_val_mae, val_mae)\n","    best_val_r2 = max(best_val_r2, val_mae)\n","\n","    # for checking\n","    diff_y = [i-j for i, j in zip(all_val_labels, all_val_preds)]\n","    check_mae = np.mean(np.abs(diff_y))\n","    # print(\"MAE Score:\", mae)\n","\n","    # early stopping\n","    early_stopping(val_loss, model)\n","    if early_stopping.early_stop:\n","        break\n","\n","    if val_mae > best_val_mae:\n","        print(f\"New best model for val mae : {val_mae:4.4f}! saving the best model..\")\n","        torch.save(model.state_dict(), f\"{save_dir}/best.pth\")\n","        best_val_mae = val_mae\n","    torch.save(model.state_dict(), f\"{save_dir}/last.pth\")\n","    print(\n","        f\"[Val] loss : {val_loss:4.4} mae : {val_mae:4.4f}, mse : {val_mse:4.4f}, rmse : {val_rmse:4.4f}, r2_score : {val_r2:4.4f} || (mae for check : {check_mae: 4.4f})\"\n","        f\"Best loss : {best_val_loss:4.4}, Best mae : {best_val_mae:4.4f}, Best r2_score : {best_val_r2:4.4f}\"\n","    )\n","    logger.add_scalar(\"Val/loss\", val_loss, epoch)\n","    logger.add_scalar(\"Val/mae\", val_mae, epoch)\n","    logger.add_scalar(\"Val/mse\", val_mse, epoch)\n","    logger.add_scalar(\"Val/rmse\", val_rmse, epoch)\n","    logger.add_scalar(\"Val/r2_score\", val_r2, epoch)\n","    print()\n","\n","    # logging wandb valid phase\n","    wandb.log({\n","        'Valid loss': val_loss,\n","        'Valid val_mae': val_mae,\n","        'Valid mse': val_mse,\n","        'Valid rmse': val_rmse,\n","        'Valid r2': val_r2,\n","    })\n","\n","wandb.finish()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sLMmLrbNETZx"},"outputs":[],"source":["with torch.no_grad():\n","\n","    model.eval()\n","\n","    test_loss_items = []\n","    all_test_preds = []\n","    all_test_labels = []\n","\n","    # prog_iter_test = tqdm(test_dataloader, desc=\"Testing\", leave=False)\n","    print()\n","    print(\"Testing results...\")\n","    # for batch_idx, test_batch in enumerate(prog_iter_test):\n","    for test_batch in test_dataloader:\n","        input_x, input_y = tuple(t.to(device) for t in test_batch)\n","        # input_x, input_y = tuple(t for t in test_batch)\n","        # input_x = input_x.to(device).float()\n","        # input_y = input_y.to(device).long()\n","\n","        outs = model(input_x)\n","\n","        loss_item = criterion(outs.to(torch.float32), input_y.to(torch.float32)).item() # regression\n","        \n","        test_loss_items.append(loss_item)\n","\n","        all_test_preds.extend(outs.cpu().numpy())\n","        all_test_labels.extend(input_y.cpu().numpy())\n","\n","test_loss = np.sum(test_loss_items) / len(test_dataloader)\n","test_mae = mean_absolute_error(all_test_labels, all_test_preds)\n","test_mse = mean_squared_error(all_test_labels, all_test_preds)\n","test_rmse = np.sqrt(test_mse)\n","test_r2 = r2_score(all_test_labels, all_test_preds)\n","\n","# for checking\n","diff_y = [i-j for i, j in zip(all_test_labels, all_test_preds)]\n","check_mae = np.mean(np.abs(diff_y))\n","# print(\"MAE Score:\", mae)\n","\n","print(\n","    f\"[Test] loss : {test_loss:4.4} mae : {test_mae:4.4f}, mse : {test_mse:4.4f}, rmse : {test_rmse:4.4f}, r2_score : {test_r2:4.4f} || (mae for check : {check_mae: 4.4f})\"\n",")\n","\n","output_dir = os.environ.get('SM_OUTPUT_DATA_DIR', './output')\n","save_path = os.path.join(output_dir, f'./preds.pkl')\n","pickle.dump((all_test_preds, all_test_labels), open(save_path, 'wb'))\n","print(f\"Inference Done! Inference result saved at {save_path}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1dc355d725bf447ab30d9e3a09ccb03c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2c495edb24c94bd4bfbd580e415f7b79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a1b8bbf44724051b66c67e05b43796d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ec9b8836daf466e8b3fe6025a112e78","value":1}},"3a1b8bbf44724051b66c67e05b43796d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c4ddc2c4f1442009ade4bffe4204112":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"VBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_af2d459b88cc4a4b803be72c37647c9d","IPY_MODEL_2c495edb24c94bd4bfbd580e415f7b79"],"layout":"IPY_MODEL_919d878bd3444873b38bf986a8c62713"}},"3ec9b8836daf466e8b3fe6025a112e78":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"919d878bd3444873b38bf986a8c62713":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"af2d459b88cc4a4b803be72c37647c9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"LabelModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d148eb90e03d41e1a2d2e82feea94f46","placeholder":"​","style":"IPY_MODEL_1dc355d725bf447ab30d9e3a09ccb03c","value":"0.038 MB of 0.038 MB uploaded\r"}},"d148eb90e03d41e1a2d2e82feea94f46":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
