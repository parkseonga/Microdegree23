{"cells":[{"cell_type":"markdown","metadata":{"id":"gu1mnQlMjdGU"},"source":["## Train"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JchSeTWxjhDp","executionInfo":{"status":"ok","timestamp":1702058927008,"user_tz":-540,"elapsed":2276,"user":{"displayName":"혜나","userId":"18133580631168845155"}},"outputId":"71312155-d5d6-4615-9661-675e86c0d56c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":349,"status":"ok","timestamp":1702058928737,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"TASed7rMOT9g","outputId":"07995dc5-5ac8-49c2-ec04-6752469a5716"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}],"source":["!pwd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MxNQL_h9OY9j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1702058929246,"user_tz":-540,"elapsed":6,"user":{"displayName":"혜나","userId":"18133580631168845155"}},"outputId":"86791f69-021d-404b-8e51-26ff423d2eb4"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/[Microdegree] Hypotension/code\n","/content/drive/MyDrive/[Microdegree] Hypotension/code\n"]}],"source":["%cd \"/content/drive/MyDrive/[Microdegree] Hypotension/code\"\n","!pwd"]},{"cell_type":"markdown","metadata":{"id":"-MliqGs3jdGY"},"source":["### Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OdYaDoKiNiA4"},"outputs":[],"source":["# !pip install torchsummary\n","# !pip install tensorboard\n","# !pip install wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C1qwNaX3NiA6"},"outputs":[],"source":["import numpy as np\n","from collections import Counter\n","from tqdm import tqdm\n","from matplotlib import pyplot as plt\n","from sklearn.metrics import classification_report, confusion_matrix\n","\n","import os\n","import glob\n","import re\n","import pickle\n","import multiprocessing\n","import wandb\n","import argparse\n","from datetime import datetime\n","from pathlib import Path\n","import random\n","import json\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.utils.tensorboard import SummaryWriter\n","from torchsummary import summary\n","\n","from model_resnet1d import ResNet1D\n","# from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":323,"status":"ok","timestamp":1702060775152,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"tw4DIOzDNiA7","outputId":"5b2bbaa5-f59a-48bd-9e2c-2cb7b7751f0b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Namespace(seed=42, epochs=100, batch_size=64, lr=0.001, criterion='mean_squared', log_interval=10, model='resnet1d', name='exp_2023-12-08-18:39:34', model_dir='./model_reg')\n"]}],"source":["now = datetime.now()\n","folder_name = now.strftime('%Y-%m-%d-%H:%M:%S')\n","parser = argparse.ArgumentParser()\n","\n","def seed_everything(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)  # if use multi-GPU\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","def increment_path(path, exist_ok=False):\n","    \"\"\" Automatically increment path, i.e. runs/exp --> runs/exp0, runs/exp1 etc.\n","\n","    Args:\n","        path (str or pathlib.Path): f\"{model_dir}/{args.name}\".\n","        exist_ok (bool): whether increment path (increment if False).\n","    \"\"\"\n","    path = Path(path)\n","    if (path.exists() and exist_ok) or (not path.exists()):\n","        return str(path)\n","    else:\n","        dirs = glob.glob(f\"{path}*\")\n","        matches = [re.search(rf\"%s(\\d+)\" % path.stem, d) for d in dirs]\n","        i = [int(m.groups()[0]) for m in matches if m]\n","        n = max(i) + 1 if i else 2\n","        return f\"{path}{n}\"\n","\n","def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']\n","\n","\n","_criterion_entrypoints = {\n","    'cross_entropy': nn.CrossEntropyLoss,\n","    'mean_squared': nn.MSELoss,\n","    # 'focal': FocalLoss,\n","    # 'label_smoothing': LabelSmoothingLoss,\n","    # 'f1': F1Loss\n","}\n","\n","def criterion_entrypoint(criterion_name):\n","    return _criterion_entrypoints[criterion_name]\n","\n","def is_criterion(criterion_name):\n","    return criterion_name in _criterion_entrypoints\n","\n","def create_criterion(criterion_name, **kwargs):\n","    if is_criterion(criterion_name):\n","        create_fn = criterion_entrypoint(criterion_name)\n","        criterion = create_fn(**kwargs)\n","    else:\n","        raise RuntimeError('Unknown loss (%s)' % criterion_name)\n","    return criterion\n","\n","parser.add_argument('--seed', type=int, default=42, help='random seed (default: 42)')\n","parser.add_argument('--epochs', type=int, default=100, help='number of epochs to train (default: 1)')\n","parser.add_argument('--batch_size', type=int, default=64, help='input batch size for training (default: 64)')\n","parser.add_argument('--lr', type=float, default=1e-3, help='learning rate (default: 1e-3)')\n","parser.add_argument('--criterion', type=str, default='mean_squared', help='criterion type (default: cross_entropy)')\n","parser.add_argument('--log_interval', type=int, default=10, help='how many batches to wait before logging training status')\n","parser.add_argument('--model', type=str, default='resnet1d', help='model type (default: BaseModel)')\n","parser.add_argument('--name', default='exp_'+folder_name, help='model save at {SM_MODEL_DIR}/{name}')\n","parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR', './model_reg'))\n","\n","# args = parser.parse_args()\n","args, _ = parser.parse_known_args()\n","print(args)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":346,"status":"ok","timestamp":1702060779026,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"oudZ8_lONiA9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"96d33b97-f734-4a05-80e4-44082aa7e71c"},"outputs":[{"output_type":"stream","name":"stdout","text":["./model_reg\n","model_reg/exp_2023-12-08-18:39:34\n"]}],"source":["model_dir = args.model_dir\n","save_dir = increment_path(os.path.join(model_dir, args.name))\n","\n","print(model_dir)\n","print(save_dir)"]},{"cell_type":"markdown","metadata":{"id":"Hlf3TO59jdGb"},"source":["### # DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kjo2ROfwNiA9"},"outputs":[],"source":["class PPGDataset(Dataset):\n","    def __init__(self, data, label):\n","        self.data = data\n","        self.label = label\n","\n","    def __getitem__(self, index):\n","        # return (torch.tensor(self.data[index], dtype=torch.float), torch.tensor(self.label[index], dtype=torch.long)) # torch.long\n","        return (torch.as_tensor(self.data[index], dtype=torch.float), torch.as_tensor(self.label[index], dtype=torch.long)) # torch.long\n","\n","    def __len__(self):\n","        return len(self.data)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":441,"status":"ok","timestamp":1702060784887,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"uSKfSJChNiA_","outputId":"3f48c29d-1a55-4a54-c1be-28597435d0be"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'> <class 'torch.Tensor'>\n","torch.Size([1892, 1, 3000]) torch.Size([1892, 1])\n","1 tensor([[35.9744, 36.3694, 36.7644,  ..., 45.4540, 48.6139, 52.9587]])\n","\n","<class 'torch.Tensor'> <class 'torch.Tensor'>\n","torch.Size([631, 1, 3000]) torch.Size([631, 1])\n","1 tensor([[29.2597, 28.8647, 28.0748,  ..., 26.0998, 26.0998, 26.0998]])\n","\n","<class 'torch.Tensor'> <class 'torch.Tensor'>\n","torch.Size([630, 1, 3000]) torch.Size([630, 1])\n","1 tensor([[37.9493, 37.5543, 37.1594,  ..., 44.2690, 44.2690, 43.4791]])\n","<__main__.PPGDataset object at 0x7e37d386db10>\n","<__main__.PPGDataset object at 0x7e37a30a9b10>\n","<__main__.PPGDataset object at 0x7e38f2f37100>\n"]}],"source":["# get dataset\n","with open('../data_reg/train_reg_x_cleaned.pkl', 'rb') as f:\n","    # train_X = pickle.load(f)\n","    X = pickle.load(f)\n","    train_X = np.expand_dims(X, 1)\n","    train_X = torch.tensor(train_X, dtype = torch.float32)\n","with open('../data_reg/train_reg_y_cleaned.pkl', 'rb') as f:\n","    # train_Y = pickle.load(f)\n","    Y = pickle.load(f)\n","    train_Y = np.expand_dims(Y, 1)\n","    train_Y = torch.tensor(train_Y, dtype = torch.float32)\n","\n","with open('../data_reg/valid_reg_x_cleaned.pkl', 'rb') as f:\n","    # valid_X = pickle.load(f)\n","    X = pickle.load(f)\n","    valid_X = np.expand_dims(X, 1)\n","    valid_X = torch.tensor(valid_X, dtype = torch.float32)\n","with open('../data_reg/valid_reg_y_cleaned.pkl', 'rb') as f:\n","    # valid_Y = pickle.load(f)\n","    Y = pickle.load(f)\n","    valid_Y = np.expand_dims(Y, 1)\n","    valid_Y = torch.tensor(valid_Y, dtype = torch.float32)\n","\n","with open('../data_reg/test_reg_x_cleaned.pkl', 'rb') as f:\n","    # test_X = pickle.load(f)\n","    X = pickle.load(f)\n","    test_X = np.expand_dims(X, 1)\n","    test_X = torch.tensor(test_X, dtype = torch.float32)\n","with open('../data_reg/test_reg_y_cleaned.pkl', 'rb') as f:\n","    # test_Y = pickle.load(f)\n","    Y = pickle.load(f)\n","    test_Y = np.expand_dims(Y, 1)\n","    test_Y = torch.tensor(test_Y, dtype = torch.float32)\n","\n","train_dataset = PPGDataset(train_X, train_Y)\n","val_dataset = PPGDataset(valid_X, valid_Y)\n","test_dataset = PPGDataset(test_X, test_Y)\n","\n","print(type(train_X), type(train_Y))\n","print(train_X.shape, train_Y.shape)\n","# print(len(train_X), len(train_Y))\n","print(len(train_X[0]), train_X[0])\n","# print(train_Y[:20])\n","print()\n","print(type(valid_X), type(valid_Y))\n","print(valid_X.shape, valid_Y.shape)\n","# print(len(valid_X), len(valid_Y))\n","print(len(valid_X[0]), valid_X[0])\n","# print(valid_Y[:20])\n","print()\n","print(type(test_X), type(test_Y))\n","print(test_X.shape, test_Y.shape)\n","# print(len(test_X), len(test_Y))\n","print(len(test_X[0]), test_X[0])\n","# print(test_Y[:20])\n","\n","print(train_dataset)\n","print(val_dataset)\n","print(test_dataset)\n","\n","# <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","# torch.Size([1892, 1, 3000]) torch.Size([1892, 1])\n","\n","# <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","# torch.Size([631, 1, 3000]) torch.Size([631, 1])\n","\n","# <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","# torch.Size([630, 1, 3000]) torch.Size([630, 1])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":297,"status":"ok","timestamp":1702060788528,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"W8_nv7PBNiBA","outputId":"554e7229-0674-46fe-bcef-862b2f39127d"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","<torch.utils.data.dataloader.DataLoader object at 0x7e37a2edb9a0>\n","<torch.utils.data.dataloader.DataLoader object at 0x7e37a2edb2e0>\n","<torch.utils.data.dataloader.DataLoader object at 0x7e37a2edbf10>\n"]}],"source":["use_cuda = torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=args.batch_size,\n","    num_workers=multiprocessing.cpu_count() // 2,\n","    shuffle=True,\n","    pin_memory=use_cuda,\n","    drop_last=True,\n","    )\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=args.batch_size,\n","    num_workers=multiprocessing.cpu_count() // 2,\n","    shuffle=False,\n","    pin_memory=use_cuda,\n","    drop_last=True,\n","    )\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    shuffle=False\n",")\n","\n","print(device)\n","print(train_dataloader)\n","print(val_dataloader)\n","print(test_dataloader)"]},{"cell_type":"markdown","metadata":{"id":"RQuaTyv5jdGc"},"source":["### # wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":333,"status":"ok","timestamp":1702060792808,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"rGfN2C0NNiBB","outputId":"f99c2a8b-1081-48a2-8ded-8b9e7f3b1e22"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":22}],"source":["import os\n","os.environ[\"WANDB_NOTEBOOK_NAME\"] = \"notebook name here\"\n","\n","# !pip install wandb -qqq\n","import wandb\n","wandb.login()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":126},"executionInfo":{"elapsed":2056,"status":"ok","timestamp":1702060796226,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"1mdVZjZ-NiBB","outputId":"fc9b7e5b-739b-43d8-8809-26d6bcbbf200"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.16.1"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/drive/MyDrive/[Microdegree] Hypotension/code/wandb/run-20231208_183953-gq3p7aj9</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9' target=\"_blank\">exp_2023-12-08-18:39:34resnet1d100</a></strong> to <a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension' target=\"_blank\">https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9' target=\"_blank\">https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9</a>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/html":["<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"],"text/plain":["<wandb.sdk.wandb_run.Run at 0x7e37a22411b0>"]},"metadata":{},"execution_count":23}],"source":["# -- wandb initialize with configuration\n","config={\n","    \"epochs\": args.epochs,\n","    \"batch_size\": args.batch_size,\n","    \"learning_rate\" : args.lr,\n","    \"architecture\" : args.model,\n","    \"loss\" : args.criterion\n","}\n","wandb.init(project=\"KAIST GSDS Microdegree - Hypotension\", name = str(save_dir.split('/')[-1])+str(args.model)+str(args.epochs), config=config)"]},{"cell_type":"markdown","metadata":{"id":"K6DBeMa8jdGd"},"source":["### # Model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":847,"status":"ok","timestamp":1702060804118,"user":{"displayName":"혜나","userId":"18133580631168845155"},"user_tz":-540},"id":"MD3AjihTNiBC","outputId":"50fe7c7f-13e8-4394-f097-6b0eebdf1e0a"},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1892, 1, 3000]) torch.Size([1892, 1])\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv1d-1            [-1, 128, 3000]           2,176\n","   MyConv1dPadSame-2            [-1, 128, 3000]               0\n","       BatchNorm1d-3            [-1, 128, 3000]             256\n","              ReLU-4            [-1, 128, 3000]               0\n","            Conv1d-5            [-1, 128, 3000]           8,320\n","   MyConv1dPadSame-6            [-1, 128, 3000]               0\n","       BatchNorm1d-7            [-1, 128, 3000]             256\n","              ReLU-8            [-1, 128, 3000]               0\n","           Dropout-9            [-1, 128, 3000]               0\n","           Conv1d-10            [-1, 128, 3000]           8,320\n","  MyConv1dPadSame-11            [-1, 128, 3000]               0\n","       BasicBlock-12            [-1, 128, 3000]               0\n","      BatchNorm1d-13            [-1, 128, 3000]             256\n","             ReLU-14            [-1, 128, 3000]               0\n","          Dropout-15            [-1, 128, 3000]               0\n","           Conv1d-16            [-1, 128, 1500]           8,320\n","  MyConv1dPadSame-17            [-1, 128, 1500]               0\n","      BatchNorm1d-18            [-1, 128, 1500]             256\n","             ReLU-19            [-1, 128, 1500]               0\n","          Dropout-20            [-1, 128, 1500]               0\n","           Conv1d-21            [-1, 128, 1500]           8,320\n","  MyConv1dPadSame-22            [-1, 128, 1500]               0\n","        MaxPool1d-23            [-1, 128, 1500]               0\n","MyMaxPool1dPadSame-24            [-1, 128, 1500]               0\n","       BasicBlock-25            [-1, 128, 1500]               0\n","      BatchNorm1d-26            [-1, 128, 1500]             256\n","             ReLU-27            [-1, 128, 1500]               0\n","          Dropout-28            [-1, 128, 1500]               0\n","           Conv1d-29            [-1, 128, 1500]           8,320\n","  MyConv1dPadSame-30            [-1, 128, 1500]               0\n","      BatchNorm1d-31            [-1, 128, 1500]             256\n","             ReLU-32            [-1, 128, 1500]               0\n","          Dropout-33            [-1, 128, 1500]               0\n","           Conv1d-34            [-1, 128, 1500]           8,320\n","  MyConv1dPadSame-35            [-1, 128, 1500]               0\n","       BasicBlock-36            [-1, 128, 1500]               0\n","      BatchNorm1d-37            [-1, 128, 1500]             256\n","             ReLU-38            [-1, 128, 1500]               0\n","          Dropout-39            [-1, 128, 1500]               0\n","           Conv1d-40             [-1, 128, 750]           8,320\n","  MyConv1dPadSame-41             [-1, 128, 750]               0\n","      BatchNorm1d-42             [-1, 128, 750]             256\n","             ReLU-43             [-1, 128, 750]               0\n","          Dropout-44             [-1, 128, 750]               0\n","           Conv1d-45             [-1, 128, 750]           8,320\n","  MyConv1dPadSame-46             [-1, 128, 750]               0\n","        MaxPool1d-47             [-1, 128, 750]               0\n","MyMaxPool1dPadSame-48             [-1, 128, 750]               0\n","       BasicBlock-49             [-1, 128, 750]               0\n","      BatchNorm1d-50             [-1, 128, 750]             256\n","             ReLU-51             [-1, 128, 750]               0\n","          Dropout-52             [-1, 128, 750]               0\n","           Conv1d-53             [-1, 256, 750]          16,640\n","  MyConv1dPadSame-54             [-1, 256, 750]               0\n","      BatchNorm1d-55             [-1, 256, 750]             512\n","             ReLU-56             [-1, 256, 750]               0\n","          Dropout-57             [-1, 256, 750]               0\n","           Conv1d-58             [-1, 256, 750]          33,024\n","  MyConv1dPadSame-59             [-1, 256, 750]               0\n","       BasicBlock-60             [-1, 256, 750]               0\n","      BatchNorm1d-61             [-1, 256, 750]             512\n","             ReLU-62             [-1, 256, 750]               0\n","          Dropout-63             [-1, 256, 750]               0\n","           Conv1d-64             [-1, 256, 375]          33,024\n","  MyConv1dPadSame-65             [-1, 256, 375]               0\n","      BatchNorm1d-66             [-1, 256, 375]             512\n","             ReLU-67             [-1, 256, 375]               0\n","          Dropout-68             [-1, 256, 375]               0\n","           Conv1d-69             [-1, 256, 375]          33,024\n","  MyConv1dPadSame-70             [-1, 256, 375]               0\n","        MaxPool1d-71             [-1, 256, 375]               0\n","MyMaxPool1dPadSame-72             [-1, 256, 375]               0\n","       BasicBlock-73             [-1, 256, 375]               0\n","      BatchNorm1d-74             [-1, 256, 375]             512\n","             ReLU-75             [-1, 256, 375]               0\n","          Dropout-76             [-1, 256, 375]               0\n","           Conv1d-77             [-1, 256, 375]          33,024\n","  MyConv1dPadSame-78             [-1, 256, 375]               0\n","      BatchNorm1d-79             [-1, 256, 375]             512\n","             ReLU-80             [-1, 256, 375]               0\n","          Dropout-81             [-1, 256, 375]               0\n","           Conv1d-82             [-1, 256, 375]          33,024\n","  MyConv1dPadSame-83             [-1, 256, 375]               0\n","       BasicBlock-84             [-1, 256, 375]               0\n","      BatchNorm1d-85             [-1, 256, 375]             512\n","             ReLU-86             [-1, 256, 375]               0\n","          Dropout-87             [-1, 256, 375]               0\n","           Conv1d-88             [-1, 256, 188]          33,024\n","  MyConv1dPadSame-89             [-1, 256, 188]               0\n","      BatchNorm1d-90             [-1, 256, 188]             512\n","             ReLU-91             [-1, 256, 188]               0\n","          Dropout-92             [-1, 256, 188]               0\n","           Conv1d-93             [-1, 256, 188]          33,024\n","  MyConv1dPadSame-94             [-1, 256, 188]               0\n","        MaxPool1d-95             [-1, 256, 188]               0\n","MyMaxPool1dPadSame-96             [-1, 256, 188]               0\n","       BasicBlock-97             [-1, 256, 188]               0\n","      BatchNorm1d-98             [-1, 256, 188]             512\n","             ReLU-99             [-1, 256, 188]               0\n","         Dropout-100             [-1, 256, 188]               0\n","          Conv1d-101             [-1, 512, 188]          66,048\n"," MyConv1dPadSame-102             [-1, 512, 188]               0\n","     BatchNorm1d-103             [-1, 512, 188]           1,024\n","            ReLU-104             [-1, 512, 188]               0\n","         Dropout-105             [-1, 512, 188]               0\n","          Conv1d-106             [-1, 512, 188]         131,584\n"," MyConv1dPadSame-107             [-1, 512, 188]               0\n","      BasicBlock-108             [-1, 512, 188]               0\n","     BatchNorm1d-109             [-1, 512, 188]           1,024\n","            ReLU-110             [-1, 512, 188]               0\n","         Dropout-111             [-1, 512, 188]               0\n","          Conv1d-112              [-1, 512, 94]         131,584\n"," MyConv1dPadSame-113              [-1, 512, 94]               0\n","     BatchNorm1d-114              [-1, 512, 94]           1,024\n","            ReLU-115              [-1, 512, 94]               0\n","         Dropout-116              [-1, 512, 94]               0\n","          Conv1d-117              [-1, 512, 94]         131,584\n"," MyConv1dPadSame-118              [-1, 512, 94]               0\n","       MaxPool1d-119              [-1, 512, 94]               0\n","MyMaxPool1dPadSame-120              [-1, 512, 94]               0\n","      BasicBlock-121              [-1, 512, 94]               0\n","     BatchNorm1d-122              [-1, 512, 94]           1,024\n","            ReLU-123              [-1, 512, 94]               0\n","         Dropout-124              [-1, 512, 94]               0\n","          Conv1d-125              [-1, 512, 94]         131,584\n"," MyConv1dPadSame-126              [-1, 512, 94]               0\n","     BatchNorm1d-127              [-1, 512, 94]           1,024\n","            ReLU-128              [-1, 512, 94]               0\n","         Dropout-129              [-1, 512, 94]               0\n","          Conv1d-130              [-1, 512, 94]         131,584\n"," MyConv1dPadSame-131              [-1, 512, 94]               0\n","      BasicBlock-132              [-1, 512, 94]               0\n","     BatchNorm1d-133              [-1, 512, 94]           1,024\n","            ReLU-134              [-1, 512, 94]               0\n","         Dropout-135              [-1, 512, 94]               0\n","          Conv1d-136              [-1, 512, 47]         131,584\n"," MyConv1dPadSame-137              [-1, 512, 47]               0\n","     BatchNorm1d-138              [-1, 512, 47]           1,024\n","            ReLU-139              [-1, 512, 47]               0\n","         Dropout-140              [-1, 512, 47]               0\n","          Conv1d-141              [-1, 512, 47]         131,584\n"," MyConv1dPadSame-142              [-1, 512, 47]               0\n","       MaxPool1d-143              [-1, 512, 47]               0\n","MyMaxPool1dPadSame-144              [-1, 512, 47]               0\n","      BasicBlock-145              [-1, 512, 47]               0\n","     BatchNorm1d-146              [-1, 512, 47]           1,024\n","            ReLU-147              [-1, 512, 47]               0\n","         Dropout-148              [-1, 512, 47]               0\n","          Conv1d-149             [-1, 1024, 47]         263,168\n"," MyConv1dPadSame-150             [-1, 1024, 47]               0\n","     BatchNorm1d-151             [-1, 1024, 47]           2,048\n","            ReLU-152             [-1, 1024, 47]               0\n","         Dropout-153             [-1, 1024, 47]               0\n","          Conv1d-154             [-1, 1024, 47]         525,312\n"," MyConv1dPadSame-155             [-1, 1024, 47]               0\n","      BasicBlock-156             [-1, 1024, 47]               0\n","     BatchNorm1d-157             [-1, 1024, 47]           2,048\n","            ReLU-158             [-1, 1024, 47]               0\n","         Dropout-159             [-1, 1024, 47]               0\n","          Conv1d-160             [-1, 1024, 24]         525,312\n"," MyConv1dPadSame-161             [-1, 1024, 24]               0\n","     BatchNorm1d-162             [-1, 1024, 24]           2,048\n","            ReLU-163             [-1, 1024, 24]               0\n","         Dropout-164             [-1, 1024, 24]               0\n","          Conv1d-165             [-1, 1024, 24]         525,312\n"," MyConv1dPadSame-166             [-1, 1024, 24]               0\n","       MaxPool1d-167             [-1, 1024, 24]               0\n","MyMaxPool1dPadSame-168             [-1, 1024, 24]               0\n","      BasicBlock-169             [-1, 1024, 24]               0\n","     BatchNorm1d-170             [-1, 1024, 24]           2,048\n","            ReLU-171             [-1, 1024, 24]               0\n","         Dropout-172             [-1, 1024, 24]               0\n","          Conv1d-173             [-1, 1024, 24]         525,312\n"," MyConv1dPadSame-174             [-1, 1024, 24]               0\n","     BatchNorm1d-175             [-1, 1024, 24]           2,048\n","            ReLU-176             [-1, 1024, 24]               0\n","         Dropout-177             [-1, 1024, 24]               0\n","          Conv1d-178             [-1, 1024, 24]         525,312\n"," MyConv1dPadSame-179             [-1, 1024, 24]               0\n","      BasicBlock-180             [-1, 1024, 24]               0\n","     BatchNorm1d-181             [-1, 1024, 24]           2,048\n","            ReLU-182             [-1, 1024, 24]               0\n","         Dropout-183             [-1, 1024, 24]               0\n","          Conv1d-184             [-1, 1024, 12]         525,312\n"," MyConv1dPadSame-185             [-1, 1024, 12]               0\n","     BatchNorm1d-186             [-1, 1024, 12]           2,048\n","            ReLU-187             [-1, 1024, 12]               0\n","         Dropout-188             [-1, 1024, 12]               0\n","          Conv1d-189             [-1, 1024, 12]         525,312\n"," MyConv1dPadSame-190             [-1, 1024, 12]               0\n","       MaxPool1d-191             [-1, 1024, 12]               0\n","MyMaxPool1dPadSame-192             [-1, 1024, 12]               0\n","      BasicBlock-193             [-1, 1024, 12]               0\n","     BatchNorm1d-194             [-1, 1024, 12]           2,048\n","            ReLU-195             [-1, 1024, 12]               0\n","          Linear-196                    [-1, 1]           1,025\n","================================================================\n","Total params: 5,276,033\n","Trainable params: 5,276,033\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 156.03\n","Params size (MB): 20.13\n","Estimated Total Size (MB): 176.17\n","----------------------------------------------------------------\n"]}],"source":["# make model\n","# device_str = \"cuda\"\n","# device = torch.device(device_str if torch.cuda.is_available() else \"cpu\")\n","\n","## change the hyper-parameters for your own data\n","# (n_block, downsample_gap, increasefilter_gap) = (8, 1, 2)\n","# 34 layer (16*2+2): 16, 2, 4\n","# 98 layer (48*2+2): 48, 6, 12\n","\n","model = ResNet1D(\n","    in_channels=1, # 3000,\n","    base_filters=128, # 128, # 64 for ResNet1D, 352 for ResNeXt1D\n","    kernel_size= 16, # kernel_size,\n","    stride=2, # stride,\n","    groups=32,\n","    n_block=16, # 48, # n_block=48,\n","    n_classes=1, # 3, 4,\n","    downsample_gap=2, # 6, # downsample_gap,\n","    increasefilter_gap=4, # 12, # increasefilter_gap,\n","    use_do=True)\n","model.to(device)\n","print(train_X.shape, train_Y.shape)\n","summary(model, (train_X.shape[1], train_X.shape[2])) # device=device\n","# exit()\n","\n","model.verbose = False # True\n","optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.lr)\n","scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n","# loss_func = torch.nn.CrossEntropyLoss()\n","criterion = create_criterion(args.criterion)  # default: cross_entropy\n","\n","logger = SummaryWriter(log_dir=save_dir)\n","with open(os.path.join(save_dir, 'config.json'), 'w', encoding='utf-8') as f:\n","    json.dump(vars(args), f, ensure_ascii=False, indent=4)\n"]},{"cell_type":"markdown","metadata":{"id":"F5aG7XXdjdGe"},"source":["### # Train"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qwNiQ4xtNiBD"},"outputs":[],"source":["# for batch_idx, train_batch in enumerate(train_dataloader):\n","#     print('# ', batch_idx)"]},{"cell_type":"code","source":["# train\n","# best_val_acc = 0\n","# best_val_loss = np.inf\n","best_mae = 0\n","# for _ in tqdm(range(args.epochs), desc=\"epoch\", leave=False):\n","for epoch in range(args.epochs):\n","\n","    # train loop\n","    model.train()\n","    # loss_value = 0\n","    # matches = 0\n","    avg_cost = 0\n","    # prog_iter = tqdm(train_dataloader, desc=\"Training\", leave=False)\n","    print(\"Training...\")\n","    # for batch_idx, train_batch in enumerate(prog_iter):\n","    for batch_idx, train_batch in enumerate(train_dataloader):\n","        input_x, input_y = tuple(t.to(device) for t in train_batch)\n","        # input_x, input_y = tuple(t for t in train_batch)\n","        # input_x = input_x.to(device).float()\n","        # input_y = input_y.to(device).long()\n","\n","        optimizer.zero_grad()\n","\n","        # preds = model(input_x)\n","        outs = model(input_x)\n","        # preds = torch.argmax(outs, dim=-1)\n","        # print('outs : ', type(outs), outs.dtype, outs.shape, outs)\n","        # print('preds : ', type(preds), preds.dtype, preds.shape, preds)\n","        # print('input_y : ', type(input_y), input_y.dtype, input_y.shape, input_y)\n","\n","        # input_y = input_y.squeeze_() # classification\n","        # print('input_y (squeezed): ', type(input_y), input_y.dtype, input_y.shape, input_y)\n","\n","        # loss = loss_func(preds, input_y)\n","        # loss = criterion(outs, input_y) # classification\n","        loss = criterion(outs.to(torch.float32), input_y.to(torch.float32)) # regression\n","        # optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        # loss_value += loss.item()\n","        # matches += (preds == input_y).sum().item()\n","        # if (batch_idx + 1) % args.log_interval == 0:\n","        #     train_loss = loss_value / args.log_interval\n","        #     train_acc = matches / args.batch_size / args.log_interval\n","        #     current_lr = get_lr(optimizer)\n","        #     print(\n","        #         f\"Epoch[{epoch}/{args.epochs}]({batch_idx + 1}/{len(train_dataloader)}) || \"\n","        #         f\"training loss {train_loss:4.4} || training accuracy {train_acc:4.2%} || lr {current_lr}\"\n","        #     )\n","        #     logger.add_scalar(\"Train/loss\", train_loss, epoch * len(train_dataloader) + batch_idx)\n","        #     logger.add_scalar(\"Train/accuracy\", train_acc, epoch * len(train_dataloader) + batch_idx)\n","\n","        #     loss_value = 0\n","        #     matches = 0\n","\n","        avg_cost += loss/len(train_dataloader)\n","\n","    if epoch % args.log_interval == 0:\n","        print(f'Epoch [{epoch+1}/{args.epochs}], train loss :', '{:.4f}'.format(avg_cost))\n","\n","    # logging wandb train phase\n","    wandb.log({\n","        # 'Train acc': train_acc,\n","        # 'Train loss': train_loss\n","        'Train loss': avg_cost\n","    })\n","    # scheduler.step(_)\n","    scheduler.step(loss)\n","\n","    # val loop\n","    with torch.no_grad():\n","\n","        model.eval()\n","        # val_loss_items = []\n","        # val_acc_items = []\n","        all_probs = []\n","        all_labels = []\n","        # prog_iter_test = tqdm(val_dataloader, desc=\"Testing\", leave=False)\n","        print(\"Calculating validation results...\")\n","        # for batch_idx, val_batch in enumerate(prog_iter_test):\n","        for val_batch in val_dataloader:\n","            input_x, input_y = tuple(t.to(device) for t in val_batch)\n","            # input_x, input_y = tuple(t for t in val_batch)\n","            # input_x = input_x.to(device).float()\n","            # input_y = input_y.to(device).long()\n","\n","            # preds = model(input_x)\n","            outs = model(input_x)\n","            # preds = torch.argmax(outs, dim=-1)\n","\n","            # input_y = input_y.squeeze_()\n","\n","            # # loss_item = loss_func(preds, input_y).item()\n","            # # loss_item = criterion(outs, input_y).item() # classification\n","            # loss_item = criterion(outs.to(torch.float32), input_y.to(torch.float32)).item() # regression\n","            # acc_item = (input_y == preds).sum().item()\n","            # val_loss_items.append(loss_item)\n","            # val_acc_items.append(acc_item)\n","\n","            all_probs.extend(outs.cpu().numpy())\n","            all_labels.extend(input_y.cpu().numpy())\n","\n","    # val_loss = np.sum(val_loss_items) / len(val_dataloader)\n","    # val_acc = np.sum(val_acc_items) / len(val_dataset)\n","    # best_val_loss = min(best_val_loss, val_loss)\n","    # if val_acc > best_val_acc:\n","    #     print(f\"New best model for val accuracy : {val_acc:4.2%}! saving the best model..\")\n","    #     torch.save(model.state_dict(), f\"{save_dir}/best.pth\")\n","    #     best_val_acc = val_acc\n","    # torch.save(model.state_dict(), f\"{save_dir}/last.pth\")\n","    # print(\n","    #     f\"[Val] acc : {val_acc:4.2%}, loss: {val_loss:4.2} || \"\n","    #     f\"best acc : {best_val_acc:4.2%}, best loss: {best_val_loss:4.2}\"\n","    # )\n","    # logger.add_scalar(\"Val/loss\", val_loss, epoch)\n","    # logger.add_scalar(\"Val/accuracy\", val_acc, epoch)\n","    # print()\n","\n","    diff_y = [i-j for i, j in zip(all_labels, all_probs)]\n","    mae = np.mean(np.abs(diff_y))\n","    print(\"MAE Score:\", mae)\n","\n","    # auc_score = roc_auc_score(all_labels, all_probs)\n","    print(f'Epoch [{epoch+1}/{args.epochs}], Validation MAE: {mae}%')\n","\n","    best_mae = min(best_mae, mae)\n","    if mae > best_mae:\n","        print(f\"New best model for val mae : {mae}%! saving the best model..\")\n","        torch.save(model.state_dict(), f\"{save_dir}/best.pth\")\n","        best_mae = mae\n","    torch.save(model.state_dict(), f\"{save_dir}/last.pth\")\n","\n","    # logging wandb valid phase\n","    wandb.log({\n","        # 'Valid acc': val_acc,\n","        # 'Valid loss': val_loss\n","        'Valid MAE': mae\n","    })\n","\n","wandb.finish()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["3c4ddc2c4f1442009ade4bffe4204112","af2d459b88cc4a4b803be72c37647c9d","2c495edb24c94bd4bfbd580e415f7b79","919d878bd3444873b38bf986a8c62713","d148eb90e03d41e1a2d2e82feea94f46","1dc355d725bf447ab30d9e3a09ccb03c","3a1b8bbf44724051b66c67e05b43796d","3ec9b8836daf466e8b3fe6025a112e78"]},"id":"phNr5KmkChqI","outputId":"5503579d-08d5-487f-ac17-b5fa5143bea1","executionInfo":{"status":"ok","timestamp":1702064304889,"user_tz":-540,"elapsed":3493168,"user":{"displayName":"혜나","userId":"18133580631168845155"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training...\n","Epoch [1/100], train loss : 6286.4658\n","Calculating validation results...\n","MAE Score: 84.10971898751126\n","Epoch [1/100], Validation MAE: 84.10971898751126%\n","New best model for val mae : 84.10971898751126%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 69.8792400823699\n","Epoch [2/100], Validation MAE: 69.8792400823699%\n","Training...\n","Calculating validation results...\n","MAE Score: 36.766583363215126\n","Epoch [3/100], Validation MAE: 36.766583363215126%\n","Training...\n","Calculating validation results...\n","MAE Score: 32.82370095120536\n","Epoch [4/100], Validation MAE: 32.82370095120536%\n","Training...\n","Calculating validation results...\n","MAE Score: 20.378891507784527\n","Epoch [5/100], Validation MAE: 20.378891507784527%\n","Training...\n","Calculating validation results...\n","MAE Score: 15.913056956397163\n","Epoch [6/100], Validation MAE: 15.913056956397163%\n","Training...\n","Calculating validation results...\n","MAE Score: 13.480724785063002\n","Epoch [7/100], Validation MAE: 13.480724785063002%\n","Training...\n","Calculating validation results...\n","MAE Score: 13.057705892456902\n","Epoch [8/100], Validation MAE: 13.057705892456902%\n","Training...\n","Calculating validation results...\n","MAE Score: 15.049186759524876\n","Epoch [9/100], Validation MAE: 15.049186759524876%\n","New best model for val mae : 15.049186759524876%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 13.429688334465027\n","Epoch [10/100], Validation MAE: 13.429688334465027%\n","Training...\n","Epoch [11/100], train loss : 196.8203\n","Calculating validation results...\n","MAE Score: 14.88918572001987\n","Epoch [11/100], Validation MAE: 14.88918572001987%\n","New best model for val mae : 14.88918572001987%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 13.13845960299174\n","Epoch [12/100], Validation MAE: 13.13845960299174%\n","Training...\n","Calculating validation results...\n","MAE Score: 13.885652038786146\n","Epoch [13/100], Validation MAE: 13.885652038786146%\n","New best model for val mae : 13.885652038786146%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 14.263565262158712\n","Epoch [14/100], Validation MAE: 14.263565262158712%\n","New best model for val mae : 14.263565262158712%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 12.458024409082201\n","Epoch [15/100], Validation MAE: 12.458024409082201%\n","Training...\n","Calculating validation results...\n","MAE Score: 14.939468675189548\n","Epoch [16/100], Validation MAE: 14.939468675189548%\n","New best model for val mae : 14.939468675189548%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 14.613810168372261\n","Epoch [17/100], Validation MAE: 14.613810168372261%\n","Training...\n","Calculating validation results...\n","MAE Score: 13.89321800072988\n","Epoch [18/100], Validation MAE: 13.89321800072988%\n","Training...\n","Calculating validation results...\n","MAE Score: 10.249209430482653\n","Epoch [19/100], Validation MAE: 10.249209430482653%\n","Training...\n","Calculating validation results...\n","MAE Score: 10.244052569071451\n","Epoch [20/100], Validation MAE: 10.244052569071451%\n","Training...\n","Epoch [21/100], train loss : 166.1488\n","Calculating validation results...\n","MAE Score: 11.99291635884179\n","Epoch [21/100], Validation MAE: 11.99291635884179%\n","New best model for val mae : 11.99291635884179%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 12.065079000261095\n","Epoch [22/100], Validation MAE: 12.065079000261095%\n","New best model for val mae : 12.065079000261095%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 10.810319264729818\n","Epoch [23/100], Validation MAE: 10.810319264729818%\n","Training...\n","Calculating validation results...\n","MAE Score: 14.21470918920305\n","Epoch [24/100], Validation MAE: 14.21470918920305%\n","New best model for val mae : 14.21470918920305%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 13.36960122320387\n","Epoch [25/100], Validation MAE: 13.36960122320387%\n","Training...\n","Calculating validation results...\n","MAE Score: 12.935689873165554\n","Epoch [26/100], Validation MAE: 12.935689873165554%\n","Training...\n","Calculating validation results...\n","MAE Score: 10.644434571266174\n","Epoch [27/100], Validation MAE: 10.644434571266174%\n","Training...\n","Calculating validation results...\n","MAE Score: 11.029808561007181\n","Epoch [28/100], Validation MAE: 11.029808561007181%\n","New best model for val mae : 11.029808561007181%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 11.557205849223667\n","Epoch [29/100], Validation MAE: 11.557205849223667%\n","New best model for val mae : 11.557205849223667%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 10.996511406368679\n","Epoch [30/100], Validation MAE: 10.996511406368679%\n","Training...\n","Epoch [31/100], train loss : 150.8138\n","Calculating validation results...\n","MAE Score: 13.15110574828254\n","Epoch [31/100], Validation MAE: 13.15110574828254%\n","New best model for val mae : 13.15110574828254%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.06830526722802\n","Epoch [32/100], Validation MAE: 9.06830526722802%\n","Training...\n","Calculating validation results...\n","MAE Score: 11.601064893934462\n","Epoch [33/100], Validation MAE: 11.601064893934462%\n","New best model for val mae : 11.601064893934462%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 10.350927419132656\n","Epoch [34/100], Validation MAE: 10.350927419132656%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.454627209239536\n","Epoch [35/100], Validation MAE: 9.454627209239536%\n","Training...\n","Calculating validation results...\n","MAE Score: 10.38137580288781\n","Epoch [36/100], Validation MAE: 10.38137580288781%\n","New best model for val mae : 10.38137580288781%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 10.656381991174486\n","Epoch [37/100], Validation MAE: 10.656381991174486%\n","New best model for val mae : 10.656381991174486%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.458970414267647\n","Epoch [38/100], Validation MAE: 9.458970414267647%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.939447707600063\n","Epoch [39/100], Validation MAE: 9.939447707600063%\n","New best model for val mae : 9.939447707600063%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 10.071417755550808\n","Epoch [40/100], Validation MAE: 10.071417755550808%\n","New best model for val mae : 10.071417755550808%! saving the best model..\n","Training...\n","Epoch [41/100], train loss : 131.1739\n","Calculating validation results...\n","MAE Score: 9.746743546591865\n","Epoch [41/100], Validation MAE: 9.746743546591865%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.859909998046028\n","Epoch [42/100], Validation MAE: 9.859909998046028%\n","New best model for val mae : 9.859909998046028%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.983744449085659\n","Epoch [43/100], Validation MAE: 9.983744449085659%\n","New best model for val mae : 9.983744449085659%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.635545293490091\n","Epoch [44/100], Validation MAE: 9.635545293490091%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.417067249615988\n","Epoch [45/100], Validation MAE: 9.417067249615988%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.825325859917534\n","Epoch [46/100], Validation MAE: 9.825325859917534%\n","New best model for val mae : 9.825325859917534%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.846565696928236\n","Epoch [47/100], Validation MAE: 9.846565696928236%\n","New best model for val mae : 9.846565696928236%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.254593676990932\n","Epoch [48/100], Validation MAE: 9.254593676990932%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.563244938850403\n","Epoch [49/100], Validation MAE: 9.563244938850403%\n","New best model for val mae : 9.563244938850403%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.155359188715616\n","Epoch [50/100], Validation MAE: 9.155359188715616%\n","Training...\n","Epoch [51/100], train loss : 123.5103\n","Calculating validation results...\n","MAE Score: 9.227864199214512\n","Epoch [51/100], Validation MAE: 9.227864199214512%\n","New best model for val mae : 9.227864199214512%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.108515699704489\n","Epoch [52/100], Validation MAE: 9.108515699704489%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.53839353720347\n","Epoch [53/100], Validation MAE: 9.53839353720347%\n","New best model for val mae : 9.53839353720347%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.131735589769152\n","Epoch [54/100], Validation MAE: 9.131735589769152%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.199793285793728\n","Epoch [55/100], Validation MAE: 9.199793285793728%\n","New best model for val mae : 9.199793285793728%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.063706768883598\n","Epoch [56/100], Validation MAE: 9.063706768883598%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.044806016816032\n","Epoch [57/100], Validation MAE: 9.044806016816032%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.104847894774544\n","Epoch [58/100], Validation MAE: 9.104847894774544%\n","New best model for val mae : 9.104847894774544%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.184740821520487\n","Epoch [59/100], Validation MAE: 9.184740821520487%\n","New best model for val mae : 9.184740821520487%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.313554180992973\n","Epoch [60/100], Validation MAE: 9.313554180992973%\n","New best model for val mae : 9.313554180992973%! saving the best model..\n","Training...\n","Epoch [61/100], train loss : 121.9181\n","Calculating validation results...\n","MAE Score: 9.271241108576456\n","Epoch [61/100], Validation MAE: 9.271241108576456%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.076962060398525\n","Epoch [62/100], Validation MAE: 9.076962060398525%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.167412095599705\n","Epoch [63/100], Validation MAE: 9.167412095599705%\n","New best model for val mae : 9.167412095599705%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.149202797147963\n","Epoch [64/100], Validation MAE: 9.149202797147963%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.379401763280233\n","Epoch [65/100], Validation MAE: 9.379401763280233%\n","New best model for val mae : 9.379401763280233%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.228011303477818\n","Epoch [66/100], Validation MAE: 9.228011303477818%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.053865975803799\n","Epoch [67/100], Validation MAE: 9.053865975803799%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.144723415374756\n","Epoch [68/100], Validation MAE: 9.144723415374756%\n","New best model for val mae : 9.144723415374756%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.045037587483725\n","Epoch [69/100], Validation MAE: 9.045037587483725%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.130199140972561\n","Epoch [70/100], Validation MAE: 9.130199140972561%\n","New best model for val mae : 9.130199140972561%! saving the best model..\n","Training...\n","Epoch [71/100], train loss : 126.5199\n","Calculating validation results...\n","MAE Score: 9.314816819296944\n","Epoch [71/100], Validation MAE: 9.314816819296944%\n","New best model for val mae : 9.314816819296944%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.332554552290174\n","Epoch [72/100], Validation MAE: 9.332554552290174%\n","New best model for val mae : 9.332554552290174%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.308796286582947\n","Epoch [73/100], Validation MAE: 9.308796286582947%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.212999184926352\n","Epoch [74/100], Validation MAE: 9.212999184926352%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.054199483659533\n","Epoch [75/100], Validation MAE: 9.054199483659533%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.11058313316769\n","Epoch [76/100], Validation MAE: 9.11058313316769%\n","New best model for val mae : 9.11058313316769%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.082700398233202\n","Epoch [77/100], Validation MAE: 9.082700398233202%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.1376979748408\n","Epoch [78/100], Validation MAE: 9.1376979748408%\n","New best model for val mae : 9.1376979748408%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.09053815735711\n","Epoch [79/100], Validation MAE: 9.09053815735711%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.149056474367777\n","Epoch [80/100], Validation MAE: 9.149056474367777%\n","New best model for val mae : 9.149056474367777%! saving the best model..\n","Training...\n","Epoch [81/100], train loss : 127.7356\n","Calculating validation results...\n","MAE Score: 9.0073178741667\n","Epoch [81/100], Validation MAE: 9.0073178741667%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.090797026952108\n","Epoch [82/100], Validation MAE: 9.090797026952108%\n","New best model for val mae : 9.090797026952108%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.060466011365255\n","Epoch [83/100], Validation MAE: 9.060466011365255%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.108211437861124\n","Epoch [84/100], Validation MAE: 9.108211437861124%\n","New best model for val mae : 9.108211437861124%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 8.997811979717678\n","Epoch [85/100], Validation MAE: 8.997811979717678%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.053421974182129\n","Epoch [86/100], Validation MAE: 9.053421974182129%\n","New best model for val mae : 9.053421974182129%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.083956440289816\n","Epoch [87/100], Validation MAE: 9.083956440289816%\n","New best model for val mae : 9.083956440289816%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.063744121127659\n","Epoch [88/100], Validation MAE: 9.063744121127659%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.120618833435906\n","Epoch [89/100], Validation MAE: 9.120618833435906%\n","New best model for val mae : 9.120618833435906%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 8.913415445221794\n","Epoch [90/100], Validation MAE: 8.913415445221794%\n","Training...\n","Epoch [91/100], train loss : 123.6708\n","Calculating validation results...\n","MAE Score: 9.19225968254937\n","Epoch [91/100], Validation MAE: 9.19225968254937%\n","New best model for val mae : 9.19225968254937%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 8.976656198501587\n","Epoch [92/100], Validation MAE: 8.976656198501587%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.042062520980835\n","Epoch [93/100], Validation MAE: 9.042062520980835%\n","New best model for val mae : 9.042062520980835%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.14920261171129\n","Epoch [94/100], Validation MAE: 9.14920261171129%\n","New best model for val mae : 9.14920261171129%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.147049175368416\n","Epoch [95/100], Validation MAE: 9.147049175368416%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.118145373132494\n","Epoch [96/100], Validation MAE: 9.118145373132494%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.051592032114664\n","Epoch [97/100], Validation MAE: 9.051592032114664%\n","Training...\n","Calculating validation results...\n","MAE Score: 9.070577383041382\n","Epoch [98/100], Validation MAE: 9.070577383041382%\n","New best model for val mae : 9.070577383041382%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.145635220739576\n","Epoch [99/100], Validation MAE: 9.145635220739576%\n","New best model for val mae : 9.145635220739576%! saving the best model..\n","Training...\n","Calculating validation results...\n","MAE Score: 9.254198339250353\n","Epoch [100/100], Validation MAE: 9.254198339250353%\n","New best model for val mae : 9.254198339250353%! saving the best model..\n"]},{"output_type":"display_data","data":{"text/plain":["VBox(children=(Label(value='0.000 MB of 0.038 MB uploaded\\r'), FloatProgress(value=0.0, max=1.0)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c4ddc2c4f1442009ade4bffe4204112"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<style>\n","    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n","    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n","    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n","    </style>\n","<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train loss</td><td>█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Valid MAE</td><td>█▄▂▁▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train loss</td><td>123.97903</td></tr><tr><td>Valid MAE</td><td>9.2542</td></tr></table><br/></div></div>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run <strong style=\"color:#cdcd00\">exp_2023-12-08-18:39:34resnet1d100</strong> at: <a href='https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9' target=\"_blank\">https://wandb.ai/hyenagatha02/KAIST%20GSDS%20Microdegree%20-%20Hypotension/runs/gq3p7aj9</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Find logs at: <code>./wandb/run-20231208_183953-gq3p7aj9/logs</code>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"sLMmLrbNETZx"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"3c4ddc2c4f1442009ade4bffe4204112":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_af2d459b88cc4a4b803be72c37647c9d","IPY_MODEL_2c495edb24c94bd4bfbd580e415f7b79"],"layout":"IPY_MODEL_919d878bd3444873b38bf986a8c62713"}},"af2d459b88cc4a4b803be72c37647c9d":{"model_module":"@jupyter-widgets/controls","model_name":"LabelModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"LabelModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"LabelView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d148eb90e03d41e1a2d2e82feea94f46","placeholder":"​","style":"IPY_MODEL_1dc355d725bf447ab30d9e3a09ccb03c","value":"0.038 MB of 0.038 MB uploaded\r"}},"2c495edb24c94bd4bfbd580e415f7b79":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a1b8bbf44724051b66c67e05b43796d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ec9b8836daf466e8b3fe6025a112e78","value":1}},"919d878bd3444873b38bf986a8c62713":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d148eb90e03d41e1a2d2e82feea94f46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dc355d725bf447ab30d9e3a09ccb03c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3a1b8bbf44724051b66c67e05b43796d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ec9b8836daf466e8b3fe6025a112e78":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}}}}},"nbformat":4,"nbformat_minor":0}